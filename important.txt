================================================================================
IMPORTANT: Installation and Version Requirements for Drug Interaction System
================================================================================

This document contains critical information about what you need to install
and which versions work best for this project.

USER ENVIRONMENT:
- Conda Environment: torch_new
- Java Version: 10 (needs upgrade - see below)

================================================================================
⚠️  CRITICAL: JAVA VERSION ISSUE
================================================================================

PROBLEM: Java 10 is NOT compatible with PySpark!

PySpark requires one of these Java versions:
  ✓ Java 8 (1.8)
  ✓ Java 11 (LTS - RECOMMENDED)
  ✓ Java 17 (LTS)

Java 10 is NOT supported and WILL cause errors like:
  - "Unsupported class file major version"
  - Spark context initialization failures
  - HDFS connection errors

SOLUTION:
Install Java 11 (Long Term Support version):

  Option 1 - Using Conda (RECOMMENDED for torch_new environment):
    conda install -c conda-forge openjdk=11

  Option 2 - System-wide installation:
    Ubuntu/Debian:
      sudo apt-get update
      sudo apt-get install openjdk-11-jdk

    macOS:
      brew install openjdk@11

    Windows:
      Download from: https://adoptium.net/temurin/releases/

After installing, verify:
  java -version
  # Should show: openjdk version "11.x.x"

Set JAVA_HOME environment variable:
  export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
  # Add this line to your ~/.bashrc or ~/.zshrc

================================================================================
1. PYSPARK INSTALLATION (For sparknote.ipynb)
================================================================================

RECOMMENDED VERSIONS:
  PySpark: 3.3.2 or 3.4.1 (compatible with Java 11)
  Hadoop: 3.x
  Scala: 2.12.x (comes bundled with PySpark)

INSTALLATION IN torch_new CONDA ENVIRONMENT:

  # Activate your environment
  conda activate torch_new

  # Install Java 11 first (if not already installed)
  conda install -c conda-forge openjdk=11

  # Install PySpark
  conda install -c conda-forge pyspark=3.3.2

  # OR using pip
  pip install pyspark==3.3.2

  # Install additional Python dependencies
  pip install py4j==0.10.9.5

VERIFY INSTALLATION:
  python -c "import pyspark; print(pyspark.__version__)"
  # Should output: 3.3.2

  python -c "from pyspark.sql import SparkSession; spark = SparkSession.builder.master('local').appName('test').getOrCreate(); print('Spark works!'); spark.stop()"

================================================================================
2. SCALA AND SBT (For CombineDatasets.scala)
================================================================================

REQUIRED FOR: Running the Scala-based data preprocessing script

RECOMMENDED VERSIONS:
  Scala: 2.12.15
  SBT: 1.8.x
  Apache Spark: 3.3.2

INSTALLATION:

  Scala:
    conda install -c conda-forge scala=2.12.15
    # OR
    curl -fL https://github.com/coursier/launchers/raw/master/scala-cli.sh | sh

  SBT (Scala Build Tool):
    conda install -c conda-forge sbt
    # OR
    echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" | sudo tee /etc/apt/sources.list.d/sbt.list
    sudo apt-get update
    sudo apt-get install sbt

VERIFY:
  scala -version
  # Should show: Scala code runner version 2.12.15

  sbt sbtVersion
  # Should show: [info] 1.8.x

================================================================================
3. HADOOP AND HDFS (For distributed data storage)
================================================================================

REQUIRED FOR: CombineDatasets.scala and sparknote.ipynb (they use HDFS)

RECOMMENDED VERSION:
  Hadoop: 3.3.4

INSTALLATION (Single Node Setup):

  1. Download Hadoop:
     wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
     tar -xzvf hadoop-3.3.4.tar.gz
     sudo mv hadoop-3.3.4 /usr/local/hadoop

  2. Set environment variables (add to ~/.bashrc):
     export HADOOP_HOME=/usr/local/hadoop
     export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
     export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

  3. Configure Hadoop (edit $HADOOP_HOME/etc/hadoop/core-site.xml):
     <configuration>
       <property>
         <name>fs.defaultFS</name>
         <value>hdfs://localhost:9000</value>
       </property>
     </configuration>

  4. Configure HDFS (edit $HADOOP_HOME/etc/hadoop/hdfs-site.xml):
     <configuration>
       <property>
         <name>dfs.replication</name>
         <value>1</value>
       </property>
       <property>
         <name>dfs.namenode.name.dir</name>
         <value>file:///usr/local/hadoop/data/namenode</value>
       </property>
       <property>
         <name>dfs.datanode.data.dir</name>
         <value>file:///usr/local/hadoop/data/datanode</value>
       </property>
     </configuration>

  5. Format HDFS (ONLY FIRST TIME):
     hdfs namenode -format

  6. Start HDFS:
     start-dfs.sh

  7. Verify HDFS is running:
     hdfs dfs -ls /
     # OR visit: http://localhost:9870

ALTERNATIVE: Skip HDFS (simpler setup)
  If you don't want to set up HDFS, you can modify the code to use local files:
  - In CombineDatasets.scala: Change hdfs://localhost:9000 to file:///
  - In sparknote.ipynb: Change hdfs paths to local file paths

================================================================================
4. PYTORCH AND MACHINE LEARNING (For multi_model_drug_interaction_prediction.ipynb)
================================================================================

RECOMMENDED VERSIONS (for torch_new environment):
  Python: 3.8 - 3.11
  PyTorch: 2.0.1 or 2.1.0 (with CUDA support if you have GPU)
  CUDA: 11.8 or 12.1 (if using GPU)
  scikit-learn: 1.3.0
  XGBoost: 1.7.6
  Pandas: 2.0.3
  NumPy: 1.24.3

INSTALLATION IN torch_new ENVIRONMENT:

  conda activate torch_new

  # For GPU (CUDA 11.8):
  conda install pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

  # For CPU only:
  conda install pytorch==2.0.1 torchvision torchaudio cpuonly -c pytorch

  # Install ML libraries
  conda install scikit-learn=1.3.0 pandas=2.0.3 numpy=1.24.3
  pip install xgboost==1.7.6

  # Install visualization libraries
  conda install matplotlib seaborn plotly

  # Install Jupyter (if not already installed)
  conda install jupyter notebook

VERIFY:
  python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')"

================================================================================
5. COMPLETE INSTALLATION SCRIPT FOR torch_new ENVIRONMENT
================================================================================

Here's a complete script to set up everything:

#!/bin/bash
# Save this as setup_torch_new.sh and run: bash setup_torch_new.sh

# Activate environment
conda activate torch_new

# Install Java 11
echo "Installing Java 11..."
conda install -c conda-forge openjdk=11 -y

# Install PySpark
echo "Installing PySpark..."
conda install -c conda-forge pyspark=3.3.2 -y

# Install Scala (optional, for running .scala files)
echo "Installing Scala..."
conda install -c conda-forge scala=2.12.15 -y

# Install PyTorch with CUDA (change to cpuonly if no GPU)
echo "Installing PyTorch..."
conda install pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia -y

# Install ML libraries
echo "Installing ML libraries..."
conda install scikit-learn=1.3.0 pandas=2.0.3 numpy=1.24.3 -y
pip install xgboost==1.7.6

# Install visualization libraries
echo "Installing visualization libraries..."
conda install matplotlib seaborn plotly -y

# Install Jupyter
echo "Installing Jupyter..."
conda install jupyter notebook -y

# Install additional dependencies
pip install py4j==0.10.9.5

echo "Installation complete!"
echo ""
echo "Verify installations:"
echo "  java -version          # Should be 11.x"
echo "  python -c 'import pyspark; print(pyspark.__version__)'  # Should be 3.3.2"
echo "  python -c 'import torch; print(torch.__version__)'      # Should be 2.0.1"

================================================================================
6. VERSION COMPATIBILITY MATRIX
================================================================================

TESTED AND WORKING COMBINATIONS:

Configuration 1 (RECOMMENDED for GPU):
  - Java: 11.0.x
  - Python: 3.10
  - PySpark: 3.3.2
  - PyTorch: 2.0.1 + CUDA 11.8
  - XGBoost: 1.7.6
  - Scala: 2.12.15
  - Hadoop: 3.3.4

Configuration 2 (CPU-only, simpler):
  - Java: 11.0.x
  - Python: 3.10
  - PySpark: 3.3.2
  - PyTorch: 2.0.1 (CPU)
  - XGBoost: 1.7.6 (CPU)
  - Scala: 2.12.15
  - NO HADOOP (use local files instead)

Configuration 3 (Minimal, for testing):
  - Java: 11.0.x
  - Python: 3.9
  - PySpark: 3.2.4
  - PyTorch: 1.13.1
  - scikit-learn: 1.2.2
  - NO HADOOP (use local files)

================================================================================
7. TROUBLESHOOTING COMMON ISSUES
================================================================================

Issue 1: "Unsupported class file major version"
  CAUSE: Java version incompatibility
  FIX: Upgrade to Java 11
       conda activate torch_new
       conda install -c conda-forge openjdk=11

Issue 2: "JAVA_HOME is not set"
  FIX: export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
       Add to ~/.bashrc to make permanent

Issue 3: "Cannot connect to HDFS at hdfs://localhost:9000"
  FIX: Either:
       - Start HDFS: start-dfs.sh
       - OR modify code to use local files instead of HDFS

Issue 4: "CUDA out of memory"
  FIX: Reduce batch size in the PyTorch code
       OR use CPU mode: device = torch.device('cpu')

Issue 5: "Module 'pyspark' has no attribute 'sql'"
  CAUSE: Incomplete PySpark installation
  FIX: pip uninstall pyspark
       conda install -c conda-forge pyspark=3.3.2

Issue 6: "py4j.protocol.Py4JError: An error occurred while calling"
  CAUSE: Java/PySpark version mismatch
  FIX: Ensure Java 11 and PySpark 3.3.2 are installed

Issue 7: "Scala compiler version mismatch"
  CAUSE: Scala version doesn't match Spark's Scala version
  FIX: Use Scala 2.12.15 (matches Spark 3.3.2)

================================================================================
8. WHAT TO INSTALL IN WHAT ORDER
================================================================================

STEP-BY-STEP INSTALLATION ORDER:

1. Java 11 (FIRST - everything depends on this)
   conda activate torch_new
   conda install -c conda-forge openjdk=11

2. PySpark (for Spark/HDFS work)
   conda install -c conda-forge pyspark=3.3.2

3. Scala and SBT (if running .scala files)
   conda install -c conda-forge scala=2.12.15
   conda install -c conda-forge sbt

4. Hadoop/HDFS (optional, for distributed storage)
   See section 3 above for detailed steps
   OR skip and modify code for local files

5. PyTorch and ML libraries (for machine learning)
   conda install pytorch==2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
   conda install scikit-learn pandas numpy matplotlib seaborn
   pip install xgboost

6. Jupyter (for running notebooks)
   conda install jupyter notebook

7. Additional tools
   pip install py4j

================================================================================
9. QUICK START CHECKLIST
================================================================================

Before running the code, verify:

[ ] Java 11 is installed: java -version shows 11.x
[ ] JAVA_HOME is set correctly
[ ] PySpark is installed: python -c "import pyspark"
[ ] PyTorch is installed: python -c "import torch"
[ ] Conda environment 'torch_new' is activated
[ ] If using HDFS: HDFS is running (hdfs dfs -ls /)
[ ] All required datasets are present

To run the notebooks:
1. CombineDatasets.scala - Run first to prepare data
   scala CombineDatasets.scala
   OR
   sbt "runMain CombineDatasets"

2. sparknote.ipynb - PySpark MLlib models
   jupyter notebook sparknote.ipynb

3. multi_model_drug_interaction_prediction.ipynb - PyTorch/XGBoost models
   jupyter notebook multi_model_drug_interaction_prediction.ipynb

================================================================================
10. RECOMMENDED SYSTEM REQUIREMENTS
================================================================================

MINIMUM:
  - CPU: 4 cores
  - RAM: 8 GB
  - Disk: 10 GB free space
  - OS: Ubuntu 18.04+, macOS 10.14+, or Windows 10+

RECOMMENDED:
  - CPU: 8+ cores
  - RAM: 16 GB
  - Disk: 20 GB free space
  - GPU: NVIDIA GPU with 6+ GB VRAM (for CUDA acceleration)
  - OS: Ubuntu 20.04+ or macOS 11+

FOR GPU (CUDA):
  - NVIDIA GPU with compute capability 3.5+
  - CUDA Toolkit: 11.8 or 12.1
  - cuDNN: 8.x
  - NVIDIA Driver: 520+ (for CUDA 11.8) or 530+ (for CUDA 12.1)

================================================================================
11. ADDITIONAL RESOURCES
================================================================================

Official Documentation:
  - PySpark: https://spark.apache.org/docs/latest/api/python/
  - PyTorch: https://pytorch.org/docs/stable/index.html
  - XGBoost: https://xgboost.readthedocs.io/
  - Hadoop: https://hadoop.apache.org/docs/stable/

Conda Documentation:
  - https://docs.conda.io/projects/conda/en/latest/user-guide/

Java Installation:
  - Adoptium (formerly AdoptOpenJDK): https://adoptium.net/

================================================================================
END OF DOCUMENT
================================================================================

For questions or issues:
1. Check this document first
2. Verify all versions match the recommendations
3. Check Java version (most common issue!)
4. Review the troubleshooting section
5. Ensure conda environment is activated

Last Updated: 2025
