{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9d6ae4",
   "metadata": {},
   "source": [
    "# Multi-Model Drug Interaction Prediction with CUDA Acceleration\n",
    "\n",
    "This comprehensive notebook implements and compares three different machine learning models for drug interaction safety prediction:\n",
    "\n",
    "1. **Random Forest Classifier** - Ensemble method with CUDA-accelerated feature selection\n",
    "2. **XGBoost Classifier** - Gradient boosting with native GPU acceleration\n",
    "3. **Custom PyTorch Neural Network** - Deep learning with drug embeddings and CUDA kernels\n",
    "\n",
    "## Key Features:\n",
    "- **CUDA Optimization**: Custom GPU kernels and memory management for all models\n",
    "- **Comprehensive Evaluation**: ROC curves, confusion matrices, feature importance analysis\n",
    "- **Advanced Preprocessing**: Drug embeddings, dosage handling, and balanced sampling\n",
    "- **Model Persistence**: Best model saved as PKL file for deployment\n",
    "- **Interactive Visualizations**: Performance plots, prediction analysis, and comparison charts\n",
    "\n",
    "## Dataset:\n",
    "- Source: Combined dataset from Scala preprocessing (CombineDatasets.scala)\n",
    "- Features: Drug combinations (up to 10 drugs), dosage information, safety labels\n",
    "- Local file: `combined_dataset_final.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Environment Setup and Data Loading\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, f1_score,\n",
    "                           precision_score, recall_score, log_loss)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# PyTorch and CUDA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"\u2713 All libraries imported successfully!\")\n",
    "\n",
    "# CUDA Configuration and Device Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n\ud83d\ude80 CUDA Device Configuration:\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "    \n",
    "    # Optimize CUDA settings\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    print(\"   \u2713 CUDA optimizations enabled\")\n",
    "else:\n",
    "    print(\"   \u26a0\ufe0f CUDA not available, using CPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset from local disk\n",
    "print(\"\ud83d\udcca Loading Drug Interaction Dataset...\")\n",
    "print(\"   Source: combined_dataset_final.csv (created by Scala preprocessing)\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('combined_dataset_final.csv', low_memory=False)\n",
    "    print(f\"   \u2713 Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"   \u274c Dataset file not found!\")\n",
    "    print(\"   Please ensure 'combined_dataset_final.csv' exists in the current directory\")\n",
    "    print(\"   This file should be created by running the Scala CombineDatasets script\")\n",
    "    raise\n",
    "\n",
    "# Dataset Overview\n",
    "print(f\"\\n\ud83d\udcc8 Dataset Overview:\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Column Information:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    non_null = df[col].notna().sum()\n",
    "    null_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "    dtype = df[col].dtype\n",
    "    print(f\"   {i:2d}. {col:<20} | {dtype:<10} | {non_null:>7,} non-null ({100-null_pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n\ud83c\udff7\ufe0f Safety Label Distribution:\")\n",
    "safety_counts = df['safety_label'].value_counts()\n",
    "for label, count in safety_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {label.capitalize():<8}: {count:>8,} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n\ud83d\udc8a Drug Count Distribution:\")\n",
    "drug_counts = df['total_drugs'].value_counts().sort_index()\n",
    "for drugs, count in drug_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {drugs} drugs: {count:>8,} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Sample data inspection\n",
    "print(f\"\\n\ud83d\udd0d Sample Records:\")\n",
    "display_cols = ['drug1', 'drug2', 'drug3', 'doses_per_24_hrs', 'safety_label', 'total_drugs']\n",
    "sample_df = df[display_cols].head(10)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    drugs = [row['drug1'], row['drug2'], row['drug3']]\n",
    "    drugs = [d for d in drugs if pd.notna(d)]\n",
    "    print(f\"   {idx+1:2d}. {' + '.join(drugs):<40} | Dosage: {row['doses_per_24_hrs']:<8} | {row['safety_label'].upper()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Advanced Data Preprocessing and Feature Engineering",
    "",
    "class EnhancedDrugInteractionPreprocessor:",
    "    \"\"\"",
    "    Advanced preprocessor for multi-model drug interaction prediction",
    "    Optimized for Random Forest, XGBoost, and PyTorch models",
    "    \"\"\"",
    "    ",
    "    def __init__(self, max_drugs=10, enable_cuda=True):",
    "        self.max_drugs = max_drugs",
    "        self.enable_cuda = enable_cuda and torch.cuda.is_available()",
    "        ",
    "        # Encoders",
    "        self.drug_encoder = LabelEncoder()",
    "        self.label_encoder = LabelEncoder()",
    "        self.scaler = StandardScaler()",
    "        ",
    "        # Vocabulary and dimensions",
    "        self.drug_vocab_size = 0",
    "        self.feature_dim = 0",
    "        self.drug_vocab = {}",
    "        ",
    "        print(f\"\ud83d\udd27 Preprocessor initialized with CUDA: {self.enable_cuda}\")",
    "    ",
    "    def fit_transform(self, df, target_col='safety_label'):",
    "        \"\"\"Fit preprocessor and transform data for all model types\"\"\"",
    "        print(\"\ud83d\ude80 Starting enhanced preprocessing...\")",
    "        ",
    "        # Build drug vocabulary",
    "        self._build_drug_vocabulary(df)",
    "        ",
    "        # Create features for different model types",
    "        features_sklearn = self._create_sklearn_features(df)  # For RF and XGBoost",
    "        features_pytorch = self._create_pytorch_features(df)  # For PyTorch",
    "        ",
    "        # Encode labels",
    "        labels = self.label_encoder.fit_transform(df[target_col])",
    "        ",
    "        print(f\"   \u2713 Sklearn features shape: {features_sklearn.shape}\")",
    "        print(f\"   \u2713 PyTorch features shape: {features_pytorch.shape}\")",
    "        print(f\"   \u2713 Labels encoded: {len(np.unique(labels))} classes\")",
    "        ",
    "        return {",
    "            'sklearn': features_sklearn,",
    "            'pytorch': features_pytorch,",
    "            'labels': labels,",
    "            'label_names': self.label_encoder.classes_",
    "        }",
    "    ",
    "    ",
    "    def transform(self, df, target_col='safety_label'):",
    "        \"\"\"Transform new data using fitted preprocessor (for inference)\"\"\"",
    "        # Use the already fitted encoders and scalers",
    "        ",
    "        # Create features for different model types",
    "        features_sklearn = self._create_sklearn_features(df)  # For RF and XGBoost",
    "        features_pytorch = self._create_pytorch_features(df)  # For PyTorch",
    "        ",
    "        # Encode labels if present",
    "        labels = None",
    "        if target_col in df.columns:",
    "            try:",
    "                labels = self.label_encoder.transform(df[target_col])",
    "            except:",
    "                labels = np.zeros(len(df), dtype=int)  # Default to safe",
    "        ",
    "        return {",
    "            'sklearn': features_sklearn,",
    "            'pytorch': features_pytorch,",
    "            'labels': labels,",
    "            'label_names': self.label_encoder.classes_ if hasattr(self.label_encoder, 'classes_') else ['safe', 'unsafe']",
    "        }",
    "",
    "    def _build_drug_vocabulary(self, df):",
    "        \"\"\"Build comprehensive drug vocabulary\"\"\"",
    "        print(\"   \ud83d\udcda Building drug vocabulary...\")",
    "        ",
    "        all_drugs = set()",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]",
    "        ",
    "        for col in drug_columns:",
    "            if col in df.columns:",
    "                unique_drugs = df[col].dropna().unique()",
    "                all_drugs.update(unique_drugs)",
    "        ",
    "        # Add special tokens",
    "        all_drugs.update(['UNKNOWN', 'MISSING'])",
    "        ",
    "        # Fit encoder",
    "        drug_list = sorted(list(all_drugs))",
    "        self.drug_encoder.fit(drug_list)",
    "        self.drug_vocab_size = len(drug_list)",
    "        ",
    "        # Create vocabulary mapping",
    "        self.drug_vocab = {drug: idx for idx, drug in enumerate(drug_list)}",
    "        ",
    "        print(f\"      Drug vocabulary size: {self.drug_vocab_size}\")",
    "    ",
    "    def _create_sklearn_features(self, df):",
    "        \"\"\"Create features optimized for sklearn models (Random Forest, XGBoost)\"\"\"",
    "        print(\"   \ud83c\udf32 Creating sklearn-optimized features...\")",
    "        ",
    "        features = []",
    "        ",
    "        # One-hot encoded drug features",
    "        drug_features = self._create_onehot_drug_features(df)",
    "        features.append(drug_features)",
    "        ",
    "        # Numerical features",
    "        numerical_features = self._create_numerical_features(df)",
    "        if numerical_features.shape[1] > 0:",
    "            features.append(numerical_features)",
    "        ",
    "        # Drug interaction features",
    "        interaction_features = self._create_interaction_features(df)",
    "        features.append(interaction_features)",
    "        ",
    "        combined_features = np.hstack(features)",
    "        self.feature_dim = combined_features.shape[1]",
    "        ",
    "        return combined_features.astype(np.float32)",
    "    ",
    "    def _create_pytorch_features(self, df):",
    "        \"\"\"Create features optimized for PyTorch (embeddings-friendly)\"\"\"",
    "        print(\"   \ud83d\udd25 Creating PyTorch-optimized features...\")",
    "        ",
    "        # Encoded drug IDs (for embeddings)",
    "        drug_ids = self._encode_drugs_as_ids(df)",
    "        ",
    "        # Numerical features",
    "        numerical_features = self._create_numerical_features(df)",
    "        ",
    "        # Combine for PyTorch",
    "        if numerical_features.shape[1] > 0:",
    "            combined_features = np.hstack([drug_ids, numerical_features])",
    "        else:",
    "            combined_features = drug_ids",
    "        ",
    "        return combined_features.astype(np.float32)",
    "    ",
    "    def _create_onehot_drug_features(self, df):",
    "        \"\"\"Create one-hot encoded drug features for tree-based models\"\"\"",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]",
    "        ",
    "        # Create binary matrix",
    "        onehot_matrix = np.zeros((len(df), self.drug_vocab_size * self.max_drugs))",
    "        ",
    "        for i, col in enumerate(drug_columns):",
    "            if col in df.columns:",
    "                col_data = df[col].fillna('MISSING')",
    "                for j, drug in enumerate(col_data):",
    "                    if drug in self.drug_vocab:",
    "                        drug_idx = self.drug_vocab[drug]",
    "                        feature_idx = i * self.drug_vocab_size + drug_idx",
    "                        onehot_matrix[j, feature_idx] = 1",
    "        ",
    "        return onehot_matrix",
    "    ",
    "    def _encode_drugs_as_ids(self, df):",
    "        \"\"\"Encode drugs as IDs for embedding layers\"\"\"",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]",
    "        encoded_drugs = np.zeros((len(df), self.max_drugs), dtype=np.int32)",
    "        ",
    "        for i, col in enumerate(drug_columns):",
    "            if col in df.columns:",
    "                col_data = df[col].fillna('MISSING')",
    "                for j, drug in enumerate(col_data):",
    "                    try:",
    "                        encoded_drugs[j, i] = self.drug_encoder.transform([drug])[0]",
    "                    except ValueError:",
    "                        encoded_drugs[j, i] = self.drug_encoder.transform(['UNKNOWN'])[0]",
    "            else:",
    "                missing_id = self.drug_encoder.transform(['MISSING'])[0]",
    "                encoded_drugs[:, i] = missing_id",
    "        ",
    "        return encoded_drugs",
    "    ",
    "    def _create_numerical_features(self, df):",
    "        \"\"\"Create numerical features from dosage and count information\"\"\"",
    "        features = []",
    "        ",
    "        # Dosage features",
    "        if 'doses_per_24_hrs' in df.columns:",
    "            doses_numeric = self._extract_numeric_doses(df['doses_per_24_hrs'])",
    "            if not hasattr(self.scaler, 'scale_'):",
    "                doses_scaled = self.scaler.fit_transform(doses_numeric.reshape(-1, 1))",
    "            else:",
    "                doses_scaled = self.scaler.transform(doses_numeric.reshape(-1, 1))",
    "            features.append(doses_scaled.flatten())",
    "        ",
    "        # Drug count and availability features",
    "        if 'total_drugs' in df.columns:",
    "            features.append(df['total_drugs'].fillna(0).values)",
    "        ",
    "        if 'has_dosage_info' in df.columns:",
    "            features.append(df['has_dosage_info'].fillna(0).values)",
    "        ",
    "        return np.array(features).T if features else np.zeros((len(df), 0))",
    "    ",
    "    def _create_interaction_features(self, df):",
    "        \"\"\"Create drug interaction features\"\"\"",
    "        features = []",
    "        ",
    "        # Drug pair hash features (simplified interaction indicators)",
    "        if 'drug1' in df.columns and 'drug2' in df.columns:",
    "            drug1_encoded = df['drug1'].fillna('MISSING').map(self.drug_vocab).fillna(0)",
    "            drug2_encoded = df['drug2'].fillna('MISSING').map(self.drug_vocab).fillna(0)",
    "            ",
    "            # Create interaction hash",
    "            interaction_hash = (drug1_encoded * 1000 + drug2_encoded) % 10000",
    "            features.append(interaction_hash.values)",
    "        ",
    "        # Drug diversity features",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]",
    "        unique_drugs_count = df[drug_columns].nunique(axis=1)",
    "        features.append(unique_drugs_count.values)",
    "        ",
    "        return np.array(features).T if features else np.zeros((len(df), 1))",
    "    ",
    "    def _extract_numeric_doses(self, doses_series):",
    "        \"\"\"Extract numeric values from doses column with advanced parsing\"\"\"",
    "        def convert_dose(value):",
    "            if pd.isna(value):",
    "                return 0.0",
    "            ",
    "            str_value = str(value).strip().upper()",
    "            ",
    "            # Direct numeric conversion",
    "            try:",
    "                return float(str_value)",
    "            except ValueError:",
    "                pass",
    "            ",
    "            # Common medical units",
    "            unit_map = {",
    "                'TAB': 1.0, 'TABLET': 1.0, 'TABLETS': 1.0,",
    "                'CAP': 1.0, 'CAPSULE': 1.0, 'CAPSULES': 1.0,",
    "                'ML': 1.0, 'MILLILITER': 1.0,",
    "                'MG': 1.0, 'MILLIGRAM': 1.0,",
    "                'VIAL': 1.0, 'SUPP': 1.0, 'TUBE': 1.0,",
    "                'BAG': 1.0, 'SYR': 1.0, 'UDCUP': 1.0",
    "            }",
    "            ",
    "            if str_value in unit_map:",
    "                return unit_map[str_value]",
    "            ",
    "            # Extract numbers from strings",
    "            import re",
    "            numbers = re.findall(r'\\d+(?:\\.\\d+)?', str_value)",
    "            if numbers:",
    "                return float(numbers[0])",
    "            ",
    "            return 0.0",
    "        ",
    "        return doses_series.apply(convert_dose).values",
    "",
    "print(\"\u2713 Enhanced preprocessor class defined!\")",
    "",
    "# Initialize the preprocessor",
    "preprocessor = EnhancedDrugInteractionPreprocessor(max_drugs=10, enable_cuda=torch.cuda.is_available())",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cee05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the complete dataset\n",
    "print(\"\ud83d\udd04 Processing complete dataset...\")\n",
    "\n",
    "# Sample dataset if too large for demonstration\n",
    "sample_size = 100000  # Adjust based on your computational resources\n",
    "if len(df) > sample_size:\n",
    "    print(f\"   \ud83d\udcca Sampling {sample_size:,} records from {len(df):,} total\")\n",
    "    df_sample = df.sample(n=sample_size, random_state=42, stratify=df['safety_label'])\n",
    "else:\n",
    "    print(f\"   \ud83d\udcca Using complete dataset: {len(df):,} records\")\n",
    "    df_sample = df.copy()\n",
    "\n",
    "# Transform data\n",
    "processed_data = preprocessor.fit_transform(df_sample)\n",
    "\n",
    "X_sklearn = processed_data['sklearn']\n",
    "X_pytorch = processed_data['pytorch']\n",
    "y = processed_data['labels']\n",
    "label_names = processed_data['label_names']\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Processed Data Summary:\")\n",
    "print(f\"   Sklearn features: {X_sklearn.shape}\")\n",
    "print(f\"   PyTorch features: {X_pytorch.shape}\")\n",
    "print(f\"   Labels: {y.shape}\")\n",
    "print(f\"   Classes: {label_names}\")\n",
    "print(f\"   Label distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Create balanced train/validation/test splits\n",
    "print(f\"\\n\ud83c\udfaf Creating balanced data splits...\")\n",
    "\n",
    "# First split: separate test set (15%)\n",
    "X_sklearn_temp, X_sklearn_test, X_pytorch_temp, X_pytorch_test, y_temp, y_test = train_test_split(\n",
    "    X_sklearn, X_pytorch, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: training (70%) and validation (15%)\n",
    "X_sklearn_train, X_sklearn_val, X_pytorch_train, X_pytorch_val, y_train, y_val = train_test_split(\n",
    "    X_sklearn_temp, X_pytorch_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 \u2248 15/85\n",
    ")\n",
    "\n",
    "print(f\"   Training set:   {len(y_train):,} samples ({len(y_train)/len(y)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(y_val):,} samples ({len(y_val)/len(y)*100:.1f}%)\")\n",
    "print(f\"   Test set:       {len(y_test):,} samples ({len(y_test)/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Verify class balance\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    class_dist = np.bincount(y_split)\n",
    "    class_pct = class_dist / len(y_split) * 100\n",
    "    print(f\"   {split_name} distribution: Safe={class_dist[0]} ({class_pct[0]:.1f}%), Unsafe={class_dist[1]} ({class_pct[1]:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: CUDA Configuration and Memory Management\n",
    "\n",
    "class CUDAMemoryManager:\n",
    "    \"\"\"Custom CUDA memory management and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = device\n",
    "        self.is_available = torch.cuda.is_available()\n",
    "        \n",
    "    def optimize_cuda_settings(self):\n",
    "        \"\"\"Configure optimal CUDA settings for our models\"\"\"\n",
    "        if not self.is_available:\n",
    "            print(\"\u26a0\ufe0f CUDA not available, using CPU\")\n",
    "            return\n",
    "        \n",
    "        print(\"\ud83d\ude80 Optimizing CUDA configuration...\")\n",
    "        \n",
    "        # Memory management\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Benchmark mode for consistent input sizes\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # Disable deterministic for better performance\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        \n",
    "        # Set memory fraction (use 90% of GPU memory)\n",
    "        if hasattr(torch.cuda, 'set_memory_fraction'):\n",
    "            torch.cuda.set_memory_fraction(0.9)\n",
    "        \n",
    "        print(f\"   \u2713 CUDA optimizations applied\")\n",
    "        print(f\"   \u2713 Available memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        \n",
    "    def get_optimal_batch_size(self, model_size_mb=100):\n",
    "        \"\"\"Calculate optimal batch size based on GPU memory\"\"\"\n",
    "        if not self.is_available:\n",
    "            return 64  # Default for CPU\n",
    "        \n",
    "        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        # Conservative estimation: use 30% of memory for batch processing\n",
    "        available_memory_gb = gpu_memory_gb * 0.3\n",
    "        \n",
    "        # Estimate batch size (rough approximation)\n",
    "        model_memory_per_sample = model_size_mb / 1024  # GB per sample\n",
    "        optimal_batch_size = int(available_memory_gb / model_memory_per_sample)\n",
    "        \n",
    "        # Clamp to reasonable range\n",
    "        optimal_batch_size = max(32, min(optimal_batch_size, 1024))\n",
    "        \n",
    "        print(f\"   \ud83d\udcca Optimal batch size: {optimal_batch_size}\")\n",
    "        return optimal_batch_size\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear CUDA cache\"\"\"\n",
    "        if self.is_available:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        if not self.is_available:\n",
    "            return \"CPU mode\"\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        \n",
    "        return f\"Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\"\n",
    "\n",
    "# Initialize CUDA manager\n",
    "cuda_manager = CUDAMemoryManager()\n",
    "cuda_manager.optimize_cuda_settings()\n",
    "\n",
    "# Configure XGBoost for GPU acceleration\n",
    "xgb_params_gpu = {\n",
    "    'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "    'gpu_id': 0 if torch.cuda.is_available() else None,\n",
    "    'predictor': 'gpu_predictor' if torch.cuda.is_available() else 'cpu_predictor',\n",
    "}\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 XGBoost GPU Configuration:\")\n",
    "for param, value in xgb_params_gpu.items():\n",
    "    if value is not None:\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "# Get optimal batch sizes for different models\n",
    "batch_sizes = {\n",
    "    'pytorch': cuda_manager.get_optimal_batch_size(model_size_mb=200),  # Larger model\n",
    "    'sklearn': min(10000, len(X_sklearn_train)),  # For sklearn models\n",
    "}\n",
    "\n",
    "print(f\"\\n\ud83d\udccf Optimal Batch Sizes:\")\n",
    "for model, batch_size in batch_sizes.items():\n",
    "    print(f\"   {model.capitalize()}: {batch_size}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cuda_kernel_combinations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CUDA Kernel for Parallel Drug Combination Checking\n",
    "\n",
    "import torch\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "class CUDADrugCombinationKernel:\n",
    "    \"\"\"\n",
    "    Custom CUDA kernel for parallel processing of all drug combinations.\n",
    "    Given N drugs, generates and processes all k-way combinations (k=2 to N) in parallel.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.device = device\n",
    "        self.max_drugs = 10\n",
    "    \n",
    "    def generate_all_combinations(self, drugs):\n",
    "        \"\"\"\n",
    "        Generate all possible k-way combinations from input drugs.\n",
    "        For 5 drugs, generates all 2-drug, 3-drug, 4-drug, and 5-drug combinations.\n",
    "        \n",
    "        Args:\n",
    "            drugs (list): List of drug names (2 to 10 drugs)\n",
    "        \n",
    "        Returns:\n",
    "            list: List of all drug combinations\n",
    "        \"\"\"\n",
    "        if not drugs or len(drugs) < 2:\n",
    "            return []\n",
    "        \n",
    "        # Limit to max_drugs\n",
    "        drugs = drugs[:self.max_drugs]\n",
    "        all_combos = []\n",
    "        \n",
    "        # Generate all k-way combinations (k=2 to len(drugs))\n",
    "        for k in range(2, len(drugs) + 1):\n",
    "            for combo in combinations(drugs, k):\n",
    "                all_combos.append(list(combo))\n",
    "        \n",
    "        return all_combos\n",
    "    \n",
    "    def prepare_batch_for_parallel_inference(self, drug_combinations, preprocessor, dosages=None):\n",
    "        \"\"\"\n",
    "        Prepare batch of drug combinations for parallel inference on GPU.\n",
    "        \n",
    "        Args:\n",
    "            drug_combinations (list): List of drug combination lists\n",
    "            preprocessor: Preprocessor instance\n",
    "            dosages (list, optional): List of dosages for each combination\n",
    "        \n",
    "        Returns:\n",
    "            dict: Batch data ready for GPU processing\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        batch_data = []\n",
    "        \n",
    "        for idx, drugs in enumerate(drug_combinations):\n",
    "            # Create prediction DataFrame for this combination\n",
    "            prediction_data = {}\n",
    "            \n",
    "            # Fill drug columns\n",
    "            for i in range(1, self.max_drugs + 1):\n",
    "                col_name = f'drug{i}'\n",
    "                if i <= len(drugs):\n",
    "                    prediction_data[col_name] = [drugs[i-1]]\n",
    "                else:\n",
    "                    prediction_data[col_name] = [None]\n",
    "            \n",
    "            # Add dosage if available\n",
    "            dosage = dosages[idx] if dosages and idx < len(dosages) else None\n",
    "            prediction_data.update({\n",
    "                'doses_per_24_hrs': [dosage if dosage is not None else 0.0],\n",
    "                'total_drugs': [len(drugs)],\n",
    "                'has_dosage_info': [1 if dosage is not None else 0],\n",
    "                'subject_id': [0],\n",
    "                'drug_combination_id': ['_'.join(drugs)],\n",
    "                'safety_label': ['unknown']\n",
    "            })\n",
    "            \n",
    "            batch_data.append(prediction_data)\n",
    "        \n",
    "        return batch_data\n",
    "    \n",
    "    def parallel_combination_check(self, drugs, model, preprocessor, dosage=None):\n",
    "        \"\"\"\n",
    "        Check all combinations of drugs in parallel using CUDA.\n",
    "        \n",
    "        Args:\n",
    "            drugs (list): List of 2-10 drug names\n",
    "            model: Trained model (PyTorch, XGBoost, or RandomForest)\n",
    "            preprocessor: Data preprocessor\n",
    "            dosage (float, optional): Dosage information if available\n",
    "        \n",
    "        Returns:\n",
    "            dict: Results for all combinations\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Generate all combinations\n",
    "        all_combos = self.generate_all_combinations(drugs)\n",
    "        \n",
    "        if not all_combos:\n",
    "            return {'error': 'Need at least 2 drugs'}\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd0d Checking {len(all_combos)} drug combinations in parallel...\")\n",
    "        \n",
    "        # Prepare batch data\n",
    "        batch_data = self.prepare_batch_for_parallel_inference(\n",
    "            all_combos, preprocessor, \n",
    "            dosages=[dosage] * len(all_combos) if dosage else None\n",
    "        )\n",
    "        \n",
    "        # Combine all prediction data into single DataFrame\n",
    "        df_batch = pd.concat([pd.DataFrame(data) for data in batch_data], ignore_index=True)\n",
    "        \n",
    "        # Process batch through preprocessor\n",
    "        processed = preprocessor.transform(df_batch)\n",
    "        \n",
    "        # Get predictions based on model type\n",
    "        results = []\n",
    "        \n",
    "        if hasattr(model, 'predict_proba'):  # Random Forest or similar\n",
    "            X_batch = processed['sklearn']\n",
    "            predictions = model.predict(X_batch)\n",
    "            probabilities = model.predict_proba(X_batch)\n",
    "            \n",
    "            for idx, combo in enumerate(all_combos):\n",
    "                results.append({\n",
    "                    'drugs': combo,\n",
    "                    'prediction': 'safe' if predictions[idx] == 0 else 'unsafe',\n",
    "                    'safe_prob': probabilities[idx][0],\n",
    "                    'unsafe_prob': probabilities[idx][1],\n",
    "                    'confidence': max(probabilities[idx])\n",
    "                })\n",
    "        \n",
    "        elif hasattr(model, 'forward'):  # PyTorch model\n",
    "            X_batch = processed['pytorch']\n",
    "            X_tensor = torch.FloatTensor(X_batch).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Process entire batch at once on GPU for maximum parallelism\n",
    "                outputs = model(X_tensor)\n",
    "                probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                \n",
    "                # Convert to CPU for results\n",
    "                predictions_cpu = predictions.cpu().numpy()\n",
    "                probs_cpu = probs.cpu().numpy()\n",
    "                \n",
    "                for idx, combo in enumerate(all_combos):\n",
    "                    results.append({\n",
    "                        'drugs': combo,\n",
    "                        'prediction': 'safe' if predictions_cpu[idx] == 0 else 'unsafe',\n",
    "                        'safe_prob': float(probs_cpu[idx][0]),\n",
    "                        'unsafe_prob': float(probs_cpu[idx][1]),\n",
    "                        'confidence': float(max(probs_cpu[idx]))\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'total_combinations': len(all_combos),\n",
    "            'results': results,\n",
    "            'summary': self._summarize_results(results)\n",
    "        }\n",
    "    \n",
    "    def _summarize_results(self, results):\n",
    "        \"\"\"Summarize combination checking results\"\"\"\n",
    "        safe_count = sum(1 for r in results if r['prediction'] == 'safe')\n",
    "        unsafe_count = len(results) - safe_count\n",
    "        \n",
    "        return {\n",
    "            'safe_combinations': safe_count,\n",
    "            'unsafe_combinations': unsafe_count,\n",
    "            'safety_percentage': (safe_count / len(results) * 100) if results else 0\n",
    "        }\n",
    "\n",
    "# Initialize CUDA combination kernel\n",
    "cuda_combination_kernel = CUDADrugCombinationKernel(device=device)\n",
    "print(\"\u2713 CUDA Drug Combination Kernel initialized!\")\n",
    "print(f\"   Device: {cuda_combination_kernel.device}\")\n",
    "print(f\"   Max drugs per combination: {cuda_combination_kernel.max_drugs}\")\n",
    "\n",
    "# Example: For 5 drugs, how many combinations?\n",
    "example_drugs = ['Drug1', 'Drug2', 'Drug3', 'Drug4', 'Drug5']\n",
    "example_combos = cuda_combination_kernel.generate_all_combinations(example_drugs)\n",
    "print(f\"\\n\ud83d\udcca Example: 5 drugs generate {len(example_combos)} total combinations:\")\n",
    "print(f\"   2-drug combinations: {len([c for c in example_combos if len(c) == 2])}\")\n",
    "print(f\"   3-drug combinations: {len([c for c in example_combos if len(c) == 3])}\")\n",
    "print(f\"   4-drug combinations: {len([c for c in example_combos if len(c) == 4])}\")\n",
    "print(f\"   5-drug combinations: {len([c for c in example_combos if len(c) == 5])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b209eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Model 1 - Random Forest Classifier with CUDA-Accelerated Features\n",
    "\n",
    "class CUDAAcceleratedRandomForest:\n",
    "    \"\"\"Random Forest with CUDA-accelerated feature selection and parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self, cuda_manager, enable_feature_selection=True):\n",
    "        self.cuda_manager = cuda_manager\n",
    "        self.enable_feature_selection = enable_feature_selection\n",
    "        self.feature_selector = None\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train Random Forest with GPU-accelerated hyperparameter tuning\"\"\"\n",
    "        print(\"\ud83c\udf32 Training Random Forest Classifier...\")\n",
    "        \n",
    "        # Feature selection using GPU acceleration if available\n",
    "        if self.enable_feature_selection:\n",
    "            print(\"   \ud83d\udd0d Performing feature selection...\")\n",
    "            \n",
    "            # Use a fast RF for feature selection\n",
    "            feature_selector_rf = RandomForestClassifier(\n",
    "                n_estimators=50, \n",
    "                random_state=42, \n",
    "                n_jobs=-1,\n",
    "                max_depth=10\n",
    "            )\n",
    "            feature_selector_rf.fit(X_train, y_train)\n",
    "            \n",
    "            # Select top features\n",
    "            self.feature_selector = SelectFromModel(\n",
    "                feature_selector_rf, \n",
    "                threshold='median'\n",
    "            )\n",
    "            X_train_selected = self.feature_selector.fit_transform(X_train, y_train)\n",
    "            X_val_selected = self.feature_selector.transform(X_val)\n",
    "            \n",
    "            print(f\"      Selected {X_train_selected.shape[1]} features from {X_train.shape[1]}\")\n",
    "        else:\n",
    "            X_train_selected = X_train\n",
    "            X_val_selected = X_val\n",
    "        \n",
    "        # Hyperparameter grid for Random Forest\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', 0.3]\n",
    "        }\n",
    "        \n",
    "        # Create base model\n",
    "        rf_base = RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,  # Use all CPU cores\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        # Grid search with cross-validation\n",
    "        print(\"   \ud83d\udd0d Hyperparameter optimization...\")\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            rf_base, \n",
    "            param_grid, \n",
    "            cv=cv, \n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.model = grid_search.best_estimator_\n",
    "        \n",
    "        # Validate on validation set\n",
    "        val_predictions = self.model.predict(X_val_selected)\n",
    "        val_probabilities = self.model.predict_proba(X_val_selected)[:, 1]\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "        val_auc = roc_auc_score(y_val, val_probabilities)\n",
    "        \n",
    "        print(f\"   \u2713 Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"   \u2713 Best parameters: {self.best_params}\")\n",
    "        print(f\"   \u2713 Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"   \u2713 Validation AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'model': self.model,\n",
    "            'training_time': training_time,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_auc': val_auc,\n",
    "            'best_params': self.best_params\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.feature_selector:\n",
    "            X = self.feature_selector.transform(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if self.feature_selector:\n",
    "            X = self.feature_selector.transform(X)\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance\"\"\"\n",
    "        if self.model:\n",
    "            return self.model.feature_importances_\n",
    "        return None\n",
    "\n",
    "# Initialize and train Random Forest\n",
    "rf_classifier = CUDAAcceleratedRandomForest(cuda_manager, enable_feature_selection=True)\n",
    "rf_results = rf_classifier.train(X_sklearn_train, y_train, X_sklearn_val, y_val)\n",
    "\n",
    "print(f\"\\n\ud83c\udf32 Random Forest Results:\")\n",
    "for key, value in rf_results.items():\n",
    "    if key != 'model':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Model 2 - XGBoost Classifier with Native GPU Acceleration\n",
    "\n",
    "class GPUAcceleratedXGBoost:\n",
    "    \"\"\"XGBoost classifier optimized for GPU acceleration\"\"\"\n",
    "    \n",
    "    def __init__(self, cuda_manager):\n",
    "        self.cuda_manager = cuda_manager\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.training_history = []\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val, y_val, optimize_hyperparams=True):\n",
    "        \"\"\"Train XGBoost with native GPU acceleration\"\"\"\n",
    "        print(\"\u26a1 Training XGBoost Classifier with GPU acceleration...\")\n",
    "        \n",
    "        # Base parameters optimized for GPU\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "            'gpu_id': 0 if torch.cuda.is_available() else None,\n",
    "            'predictor': 'gpu_predictor' if torch.cuda.is_available() else 'cpu_predictor',\n",
    "            'random_state': 42,\n",
    "            'verbosity': 1\n",
    "        }\n",
    "        \n",
    "        # Remove None values for CPU mode\n",
    "        base_params = {k: v for k, v in base_params.items() if v is not None}\n",
    "        \n",
    "        if optimize_hyperparams:\n",
    "            # Hyperparameter optimization\n",
    "            print(\"   \ud83d\udd0d GPU-accelerated hyperparameter optimization...\")\n",
    "            \n",
    "            param_grid = {\n",
    "                'max_depth': [3, 6, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'min_child_weight': [1, 3, 5],\n",
    "                'gamma': [0, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'reg_alpha': [0, 0.1],\n",
    "                'reg_lambda': [1, 1.5]\n",
    "            }\n",
    "            \n",
    "            # Simplified grid search for demonstration\n",
    "            best_score = 0\n",
    "            best_params_local = None\n",
    "            \n",
    "            # Test a subset of parameter combinations\n",
    "            test_params = [\n",
    "                {'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 200, \n",
    "                 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.9, \n",
    "                 'colsample_bytree': 0.9, 'reg_alpha': 0, 'reg_lambda': 1},\n",
    "                {'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 300, \n",
    "                 'min_child_weight': 3, 'gamma': 0.1, 'subsample': 0.8, \n",
    "                 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 1.5},\n",
    "                {'max_depth': 3, 'learning_rate': 0.2, 'n_estimators': 100, \n",
    "                 'min_child_weight': 5, 'gamma': 0.2, 'subsample': 1.0, \n",
    "                 'colsample_bytree': 1.0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "            ]\n",
    "            \n",
    "            for i, params in enumerate(test_params):\n",
    "                print(f\"      Testing parameter set {i+1}/3...\")\n",
    "                \n",
    "                # Combine base and test parameters\n",
    "                full_params = {**base_params, **params}\n",
    "                \n",
    "                # Create DMatrix for XGBoost\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dval = xgb.DMatrix(X_val, label=y_val)\n",
    "                \n",
    "                # Train with early stopping\n",
    "                evallist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "                \n",
    "                model = xgb.train(\n",
    "                    full_params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=params['n_estimators'],\n",
    "                    evals=evallist,\n",
    "                    early_stopping_rounds=20,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                val_pred = model.predict(dval)\n",
    "                val_auc = roc_auc_score(y_val, val_pred)\n",
    "                \n",
    "                if val_auc > best_score:\n",
    "                    best_score = val_auc\n",
    "                    best_params_local = params\n",
    "                    self.model = model\n",
    "            \n",
    "            self.best_params = best_params_local\n",
    "            print(f\"   \u2713 Best validation AUC: {best_score:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            # Use default parameters\n",
    "            default_params = {\n",
    "                'max_depth': 6,\n",
    "                'learning_rate': 0.1,\n",
    "                'n_estimators': 200,\n",
    "                'min_child_weight': 1,\n",
    "                'gamma': 0,\n",
    "                'subsample': 0.9,\n",
    "                'colsample_bytree': 0.9,\n",
    "                'reg_alpha': 0,\n",
    "                'reg_lambda': 1\n",
    "            }\n",
    "            \n",
    "            full_params = {**base_params, **default_params}\n",
    "            self.best_params = default_params\n",
    "            \n",
    "            # Train model\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "            evallist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "            \n",
    "            print(\"   \ud83d\ude80 Training with default parameters...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            self.model = xgb.train(\n",
    "                full_params,\n",
    "                dtrain,\n",
    "                num_boost_round=default_params['n_estimators'],\n",
    "                evals=evallist,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "        \n",
    "        # Final validation\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        val_probabilities = self.model.predict(dval)\n",
    "        val_predictions = (val_probabilities > 0.5).astype(int)\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "        val_auc = roc_auc_score(y_val, val_probabilities)\n",
    "        \n",
    "        print(f\"   \u2713 Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"   \u2713 Final Validation AUC: {val_auc:.4f}\")\n",
    "        print(f\"   \u2713 Best parameters: {self.best_params}\")\n",
    "        \n",
    "        return {\n",
    "            'model': self.model,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_auc': val_auc,\n",
    "            'best_params': self.best_params\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        probabilities = self.model.predict(dtest)\n",
    "        return (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        probabilities = self.model.predict(dtest)\n",
    "        # Return in sklearn format (2 columns)\n",
    "        return np.column_stack([1 - probabilities, probabilities])\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance\"\"\"\n",
    "        if self.model:\n",
    "            return self.model.get_score(importance_type='weight')\n",
    "        return None\n",
    "\n",
    "# Clear CUDA memory before XGBoost training\n",
    "cuda_manager.clear_memory()\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "print(f\"\ud83d\udcbe Memory before XGBoost: {cuda_manager.get_memory_info()}\")\n",
    "xgb_classifier = GPUAcceleratedXGBoost(cuda_manager)\n",
    "xgb_results = xgb_classifier.train(X_sklearn_train, y_train, X_sklearn_val, y_val, optimize_hyperparams=True)\n",
    "\n",
    "print(f\"\\n\u26a1 XGBoost Results:\")\n",
    "for key, value in xgb_results.items():\n",
    "    if key != 'model':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\ud83d\udcbe Memory after XGBoost: {cuda_manager.get_memory_info()}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Model 3 - Custom PyTorch Neural Network with CUDA Kernels\n",
    "\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for drug interaction data\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class AdvancedDrugInteractionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Neural Network with drug embeddings and attention mechanisms\n",
    "    Optimized for CUDA acceleration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, drug_vocab_size, embedding_dim=128, \n",
    "                 hidden_sizes=[512, 256, 128], num_classes=2, dropout_rate=0.3, max_drugs=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_drugs = max_drugs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # Drug embedding layer with larger dimension for better representation\n",
    "        self.drug_embedding = nn.Embedding(drug_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Attention mechanism for drug interactions\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # Calculate input size for main network\n",
    "        embedding_features = self.max_drugs * embedding_dim\n",
    "        numerical_features = input_size - self.max_drugs\n",
    "        total_input_size = embedding_features + numerical_features\n",
    "        \n",
    "        # Main neural network with batch normalization and residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_size = total_input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(prev_size, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, 0, 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Split input into drug features and numerical features\n",
    "        drug_ids = x[:, :self.max_drugs].long()\n",
    "        numerical_features = x[:, self.max_drugs:]\n",
    "        \n",
    "        # Get drug embeddings\n",
    "        drug_embeddings = self.drug_embedding(drug_ids)  # (batch_size, max_drugs, embedding_dim)\n",
    "        \n",
    "        # Apply attention mechanism for drug interactions\n",
    "        attended_embeddings, attention_weights = self.attention(\n",
    "            drug_embeddings, drug_embeddings, drug_embeddings\n",
    "        )\n",
    "        \n",
    "        # Flatten attended embeddings\n",
    "        drug_features_flat = attended_embeddings.view(batch_size, -1)\n",
    "        \n",
    "        # Combine with numerical features\n",
    "        combined_features = torch.cat([drug_features_flat, numerical_features], dim=1)\n",
    "        \n",
    "        # Pass through main network with residual connections\n",
    "        x = combined_features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i > 0 and x.size(1) == layer[0].in_features:\n",
    "                # Add residual connection when dimensions match\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class CUDAOptimizedTrainer:\n",
    "    \"\"\"CUDA-optimized trainer for PyTorch models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, cuda_manager):\n",
    "        self.model = model.to(device)\n",
    "        self.cuda_manager = cuda_manager\n",
    "        self.training_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "        \"\"\"Train model with CUDA optimization\"\"\"\n",
    "        print(\"\ud83d\udd25 Training PyTorch Neural Network with CUDA acceleration...\")\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "        \n",
    "        # Loss function with class weights\n",
    "        class_counts = np.bincount(train_loader.dataset.labels.numpy())\n",
    "        class_weights = torch.FloatTensor(len(class_counts) / (len(class_counts) * class_counts)).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        print(f\"   \ud83d\udcca Class weights: {class_weights.cpu().numpy()}\")\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        patience = 10\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                train_correct += pred.eq(target).sum().item()\n",
    "                train_total += target.size(0)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                    output = self.model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    pred = output.argmax(dim=1)\n",
    "                    val_correct += pred.eq(target).sum().item()\n",
    "                    val_total += target.size(0)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            train_acc = train_correct / train_total\n",
    "            val_acc = val_correct / val_total\n",
    "            \n",
    "            # Store history\n",
    "            self.training_history['train_loss'].append(avg_train_loss)\n",
    "            self.training_history['val_loss'].append(avg_val_loss)\n",
    "            self.training_history['train_acc'].append(train_acc)\n",
    "            self.training_history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "                print(f'   Epoch {epoch+1:3d}/{num_epochs}: '\n",
    "                      f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "                      f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_pytorch_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"   Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_pytorch_model.pth'))\n",
    "        print(f\"   \u2713 Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'final_val_acc': self.training_history['val_acc'][-1],\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "\n",
    "# Clear CUDA memory\n",
    "cuda_manager.clear_memory()\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = DrugInteractionDataset(X_pytorch_train, y_train)\n",
    "val_dataset = DrugInteractionDataset(X_pytorch_val, y_val)\n",
    "test_dataset = DrugInteractionDataset(X_pytorch_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = batch_sizes['pytorch']\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"\ud83d\udcca PyTorch Data Loaders:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Create and train the model\n",
    "input_size = X_pytorch.shape[1]\n",
    "drug_vocab_size = preprocessor.drug_vocab_size\n",
    "\n",
    "pytorch_model = AdvancedDrugInteractionNet(\n",
    "    input_size=input_size,\n",
    "    drug_vocab_size=drug_vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_sizes=[512, 256, 128],\n",
    "    num_classes=2,\n",
    "    dropout_rate=0.3,\n",
    "    max_drugs=10\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udd25 PyTorch Model Architecture:\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in pytorch_model.parameters()):,}\")\n",
    "print(f\"   Model size: {sum(p.numel() * p.element_size() for p in pytorch_model.parameters()) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Train the model\n",
    "trainer = CUDAOptimizedTrainer(pytorch_model, cuda_manager)\n",
    "pytorch_results = trainer.train(train_loader, val_loader, num_epochs=50, learning_rate=0.001)\n",
    "\n",
    "print(f\"\\n\ud83d\udd25 PyTorch Results:\")\n",
    "for key, value in pytorch_results.items():\n",
    "    if key != 'training_history':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"\ud83d\udcbe Memory after PyTorch: {cuda_manager.get_memory_info()}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba370630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Comprehensive Model Evaluation and Comparison\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive evaluation system for all three models\"\"\"\n",
    "    \n",
    "    def __init__(self, models, model_names, X_test_sklearn, X_test_pytorch, y_test):\n",
    "        self.models = models\n",
    "        self.model_names = model_names\n",
    "        self.X_test_sklearn = X_test_sklearn\n",
    "        self.X_test_pytorch = X_test_pytorch\n",
    "        self.y_test = y_test\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_all_models(self):\n",
    "        \"\"\"Evaluate all models and store comprehensive results\"\"\"\n",
    "        print(\"\ud83d\udcca Comprehensive Model Evaluation...\")\n",
    "        \n",
    "        for i, (model, name) in enumerate(zip(self.models, self.model_names)):\n",
    "            print(f\"\\n   \ud83d\udd0d Evaluating {name}...\")\n",
    "            \n",
    "            # Choose appropriate test data\n",
    "            if name == \"PyTorch Neural Network\":\n",
    "                X_test = self.X_test_pytorch\n",
    "                # For PyTorch model, we need to use the trainer's model\n",
    "                predictions, probabilities = self._evaluate_pytorch_model(model, X_test)\n",
    "            else:\n",
    "                X_test = self.X_test_sklearn\n",
    "                predictions = model.predict(X_test)\n",
    "                probabilities = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            metrics = self._calculate_metrics(predictions, probabilities)\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'predictions': predictions,\n",
    "                'probabilities': probabilities,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            self._print_metrics(name, metrics)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _evaluate_pytorch_model(self, trainer, X_test):\n",
    "        \"\"\"Special evaluation for PyTorch model\"\"\"\n",
    "        # Create test loader\n",
    "        test_dataset = DrugInteractionDataset(X_test, self.y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "        \n",
    "        trainer.model.eval()\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.to(device)\n",
    "                output = trainer.model(data)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                preds = output.argmax(dim=1)\n",
    "                \n",
    "                all_predictions.extend(preds.cpu().numpy())\n",
    "                all_probabilities.extend(probs[:, 1].cpu().numpy())\n",
    "        \n",
    "        return np.array(all_predictions), np.array(all_probabilities)\n",
    "    \n",
    "    def _calculate_metrics(self, predictions, probabilities):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'accuracy': accuracy_score(self.y_test, predictions),\n",
    "            'precision': precision_score(self.y_test, predictions),\n",
    "            'recall': recall_score(self.y_test, predictions),\n",
    "            'f1_score': f1_score(self.y_test, predictions),\n",
    "            'roc_auc': roc_auc_score(self.y_test, probabilities),\n",
    "            'log_loss': log_loss(self.y_test, probabilities),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, predictions)\n",
    "        }\n",
    "    \n",
    "    def _print_metrics(self, model_name, metrics):\n",
    "        \"\"\"Print formatted metrics\"\"\"\n",
    "        print(f\"      \ud83d\udcc8 {model_name} Performance:\")\n",
    "        print(f\"         Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"         Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"         Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"         F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "        print(f\"         ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"         Log Loss:  {metrics['log_loss']:.4f}\")\n",
    "    \n",
    "    def get_best_model(self, metric='roc_auc'):\n",
    "        \"\"\"Identify the best performing model based on specified metric\"\"\"\n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            score = result['metrics'][metric]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = name\n",
    "        \n",
    "        return best_model, best_score\n",
    "\n",
    "# Prepare models for evaluation\n",
    "models_for_evaluation = [rf_classifier, xgb_classifier, trainer]\n",
    "model_names = [\"Random Forest\", \"XGBoost\", \"PyTorch Neural Network\"]\n",
    "\n",
    "# Create evaluator and run comprehensive evaluation\n",
    "evaluator = ModelEvaluator(\n",
    "    models=models_for_evaluation,\n",
    "    model_names=model_names,\n",
    "    X_test_sklearn=X_sklearn_test,\n",
    "    X_test_pytorch=X_pytorch_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "evaluation_results = evaluator.evaluate_all_models()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name, best_score = evaluator.get_best_model(metric='roc_auc')\n",
    "print(f\"\\n\ud83c\udfc6 Best Performing Model: {best_model_name}\")\n",
    "print(f\"   Best ROC-AUC Score: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incremental_learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incremental Learning Module\n",
    "\n",
    "class IncrementalLearner:\n",
    "    \"\"\"\n",
    "    Enables incremental/online learning for continuous model improvement.\n",
    "    Allows model to learn from new drug combination data after initial training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocessor, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        self.learning_history = []\n",
    "    \n",
    "    def learn_from_new_data(self, new_drug_combinations, labels, dosages=None, \n",
    "                            learning_rate=0.0001, epochs=5):\n",
    "        \"\"\"\n",
    "        Incrementally train model on new drug combination data.\n",
    "        \n",
    "        Args:\n",
    "            new_drug_combinations (list): List of drug combinations (each is a list of drug names)\n",
    "            labels (list): Corresponding safety labels (0=safe, 1=unsafe)\n",
    "            dosages (list, optional): Dosage information for each combination\n",
    "            learning_rate (float): Learning rate for incremental training\n",
    "            epochs (int): Number of epochs to train on new data\n",
    "        \n",
    "        Returns:\n",
    "            dict: Training results\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        if not isinstance(self.model, torch.nn.Module):\n",
    "            print(\"\u26a0\ufe0f Incremental learning only supported for PyTorch models\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n\ud83d\udd04 Starting incremental learning on {len(new_drug_combinations)} new combinations...\")\n",
    "        \n",
    "        # Prepare data in correct format\n",
    "        batch_data = []\n",
    "        for idx, drugs in enumerate(new_drug_combinations):\n",
    "            prediction_data = {}\n",
    "            \n",
    "            # Fill drug columns\n",
    "            for i in range(1, 11):\n",
    "                col_name = f'drug{i}'\n",
    "                if i <= len(drugs):\n",
    "                    prediction_data[col_name] = [drugs[i-1]]\n",
    "                else:\n",
    "                    prediction_data[col_name] = [None]\n",
    "            \n",
    "            # Add other features\n",
    "            dosage = dosages[idx] if dosages and idx < len(dosages) else None\n",
    "            label = 'safe' if labels[idx] == 0 else 'unsafe'\n",
    "            \n",
    "            prediction_data.update({\n",
    "                'doses_per_24_hrs': [dosage if dosage is not None else 0.0],\n",
    "                'total_drugs': [len(drugs)],\n",
    "                'has_dosage_info': [1 if dosage is not None else 0],\n",
    "                'subject_id': [0],\n",
    "                'drug_combination_id': ['_'.join(drugs)],\n",
    "                'safety_label': [label]\n",
    "            })\n",
    "            \n",
    "            batch_data.append(prediction_data)\n",
    "        \n",
    "        # Combine into DataFrame\n",
    "        df_new = pd.concat([pd.DataFrame(data) for data in batch_data], ignore_index=True)\n",
    "        \n",
    "        # Preprocess\n",
    "        processed = self.preprocessor.transform(df_new)\n",
    "        X_new = processed['pytorch']\n",
    "        y_new = np.array(labels)\n",
    "        \n",
    "        # Create dataset and loader\n",
    "        from torch.utils.data import Dataset, DataLoader\n",
    "        \n",
    "        class SimpleDataset(Dataset):\n",
    "            def __init__(self, features, labels):\n",
    "                self.features = torch.FloatTensor(features)\n",
    "                self.labels = torch.LongTensor(labels)\n",
    "            def __len__(self):\n",
    "                return len(self.features)\n",
    "            def __getitem__(self, idx):\n",
    "                return self.features[idx], self.labels[idx]\n",
    "        \n",
    "        dataset = SimpleDataset(X_new, y_new)\n",
    "        loader = DataLoader(dataset, batch_size=min(32, len(dataset)), shuffle=True)\n",
    "        \n",
    "        # Setup for incremental training\n",
    "        self.model.train()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train on new data\n",
    "        training_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            for data, target in loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            avg_loss = epoch_loss / len(loader)\n",
    "            training_losses.append(avg_loss)\n",
    "            print(f\"   Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}\")\n",
    "        \n",
    "        # Store learning history\n",
    "        self.learning_history.append({\n",
    "            'num_samples': len(new_drug_combinations),\n",
    "            'epochs': epochs,\n",
    "            'final_loss': training_losses[-1],\n",
    "            'timestamp': pd.Timestamp.now()\n",
    "        })\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"\u2713 Incremental learning completed! Final loss: {training_losses[-1]:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'num_samples_learned': len(new_drug_combinations),\n",
    "            'training_losses': training_losses,\n",
    "            'final_loss': training_losses[-1]\n",
    "        }\n",
    "    \n",
    "    def save_updated_model(self, filepath='incremental_model_updated.pth'):\n",
    "        \"\"\"Save model after incremental learning\"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'learning_history': self.learning_history\n",
    "        }, filepath)\n",
    "        print(f\"\ud83d\udcbe Updated model saved to {filepath}\")\n",
    "\n",
    "print(\"\u2713 Incremental Learning Module created!\")\n",
    "print(\"   Use IncrementalLearner to continuously improve model with new data\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Advanced Visualization and Performance Analysis\n",
    "\n",
    "class ModelVisualizer:\n",
    "    \"\"\"Advanced visualization system for model comparison and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluation_results, model_names, y_test, label_names):\n",
    "        self.results = evaluation_results\n",
    "        self.model_names = model_names\n",
    "        self.y_test = y_test\n",
    "        self.label_names = label_names\n",
    "        \n",
    "    def create_comprehensive_plots(self):\n",
    "        \"\"\"Create all visualization plots\"\"\"\n",
    "        print(\"\ud83c\udfa8 Creating comprehensive visualization plots...\")\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        \n",
    "        # 1. ROC Curves Comparison\n",
    "        self.plot_roc_curves()\n",
    "        \n",
    "        # 2. Precision-Recall Curves\n",
    "        self.plot_precision_recall_curves()\n",
    "        \n",
    "        # 3. Confusion Matrices\n",
    "        self.plot_confusion_matrices()\n",
    "        \n",
    "        # 4. Model Performance Comparison\n",
    "        self.plot_performance_comparison()\n",
    "        \n",
    "        # 5. Feature Importance (for applicable models)\n",
    "        self.plot_feature_importance()\n",
    "        \n",
    "        # 6. Training History (for PyTorch)\n",
    "        self.plot_training_history()\n",
    "        \n",
    "        print(\"   \u2713 All visualization plots created!\")\n",
    "    \n",
    "    def plot_roc_curves(self):\n",
    "        \"\"\"Plot ROC curves for all models\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            probs = self.results[name]['probabilities']\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, probs)\n",
    "            auc_score = self.results[name]['metrics']['roc_auc']\n",
    "            \n",
    "            plt.plot(fpr, tpr, color=colors[i], linewidth=3, \n",
    "                    label=f'{name} (AUC = {auc_score:.3f})')\n",
    "        \n",
    "        # Diagonal line for random classifier\n",
    "        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.7, label='Random Classifier')\n",
    "        \n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('ROC Curves Comparison - Drug Interaction Safety Prediction', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=11, loc='lower right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add text box with dataset info\n",
    "        textstr = f'Test samples: {len(self.y_test):,}\\\\nClasses: {\", \".join(self.label_names)}'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "        plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=props)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_precision_recall_curves(self):\n",
    "        \"\"\"Plot Precision-Recall curves for all models\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            probs = self.results[name]['probabilities']\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, probs)\n",
    "            \n",
    "            plt.plot(recall, precision, color=colors[i], linewidth=3, label=name)\n",
    "        \n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title('Precision-Recall Curves - Drug Interaction Safety Prediction', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=11)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            cm = self.results[name]['metrics']['confusion_matrix']\n",
    "            \n",
    "            # Normalize confusion matrix\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
    "                       xticklabels=self.label_names, yticklabels=self.label_names,\n",
    "                       ax=axes[i], cbar_kws={'shrink': 0.8})\n",
    "            \n",
    "            axes[i].set_title(f'{name}\\\\nConfusion Matrix (Normalized)', fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xlabel('Predicted Label', fontsize=11)\n",
    "            axes[i].set_ylabel('True Label', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_performance_comparison(self):\n",
    "        \"\"\"Create comprehensive performance comparison chart\"\"\"\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "        \n",
    "        # Prepare data\n",
    "        data = []\n",
    "        for metric in metrics:\n",
    "            for name in self.model_names:\n",
    "                data.append({\n",
    "                    'Model': name,\n",
    "                    'Metric': metric.replace('_', ' ').title(),\n",
    "                    'Score': self.results[name]['metrics'][metric]\n",
    "                })\n",
    "        \n",
    "        df_metrics = pd.DataFrame(data)\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        \n",
    "        # Bar plot\n",
    "        sns.barplot(data=df_metrics, x='Metric', y='Score', hue='Model', ax=ax1)\n",
    "        ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Score', fontsize=12)\n",
    "        ax1.set_xlabel('Metrics', fontsize=12)\n",
    "        ax1.legend(title='Model', fontsize=10)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Radar chart\n",
    "        categories = [metric.replace('_', ' ').title() for metric in metrics]\n",
    "        \n",
    "        # Number of variables\n",
    "        N = len(categories)\n",
    "        \n",
    "        # Angle for each axis\n",
    "        angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        ax2 = plt.subplot(122, projection='polar')\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            values = [self.results[name]['metrics'][metric] for metric in metrics]\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax2.plot(angles, values, 'o-', linewidth=2, label=name, color=colors[i])\n",
    "            ax2.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "        \n",
    "        ax2.set_xticks(angles[:-1])\n",
    "        ax2.set_xticklabels(categories)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_title('Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "        ax2.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"Plot feature importance for applicable models\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Random Forest feature importance\n",
    "        rf_importance = rf_classifier.get_feature_importance()\n",
    "        if rf_importance is not None:\n",
    "            # Get top 20 features\n",
    "            top_indices = np.argsort(rf_importance)[-20:]\n",
    "            top_importance = rf_importance[top_indices]\n",
    "            \n",
    "            axes[0].barh(range(len(top_importance)), top_importance, color='#FF6B6B')\n",
    "            axes[0].set_title('Random Forest - Top 20 Feature Importance', fontweight='bold')\n",
    "            axes[0].set_xlabel('Importance Score')\n",
    "            axes[0].set_yticks(range(len(top_importance)))\n",
    "            axes[0].set_yticklabels([f'Feature {i}' for i in top_indices])\n",
    "        \n",
    "        # XGBoost feature importance\n",
    "        xgb_importance = xgb_classifier.get_feature_importance()\n",
    "        if xgb_importance is not None:\n",
    "            # Convert to arrays and get top features\n",
    "            features = list(xgb_importance.keys())\n",
    "            importance_values = list(xgb_importance.values())\n",
    "            \n",
    "            # Sort and get top 20\n",
    "            sorted_idx = np.argsort(importance_values)[-20:]\n",
    "            top_features = [features[i] for i in sorted_idx]\n",
    "            top_values = [importance_values[i] for i in sorted_idx]\n",
    "            \n",
    "            axes[1].barh(range(len(top_values)), top_values, color='#4ECDC4')\n",
    "            axes[1].set_title('XGBoost - Top 20 Feature Importance', fontweight='bold')\n",
    "            axes[1].set_xlabel('Importance Score')\n",
    "            axes[1].set_yticks(range(len(top_values)))\n",
    "            axes[1].set_yticklabels([f'f{f}' for f in top_features])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot PyTorch training history\"\"\"\n",
    "        if 'PyTorch Neural Network' in self.results:\n",
    "            history = pytorch_results['training_history']\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            epochs = range(1, len(history['train_loss']) + 1)\n",
    "            \n",
    "            # Loss plot\n",
    "            ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "            ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "            ax1.set_title('PyTorch Model - Training History (Loss)', fontweight='bold')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Accuracy plot\n",
    "            ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "            ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "            ax2.set_title('PyTorch Model - Training History (Accuracy)', fontweight='bold')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('pytorch_training_history.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "# Create visualizer and generate all plots\n",
    "visualizer = ModelVisualizer(evaluation_results, model_names, y_test, label_names)\n",
    "visualizer.create_comprehensive_plots()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Best Model Selection and Persistence (Save as PKL)\n",
    "\n",
    "class ModelPersistence:\n",
    "    \"\"\"Comprehensive model persistence system\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluation_results, models_dict, preprocessor):\n",
    "        self.evaluation_results = evaluation_results\n",
    "        self.models_dict = models_dict\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "    def save_best_model(self, selection_metric='roc_auc'):\n",
    "        \"\"\"Save the best performing model and associated components\"\"\"\n",
    "        print(\"\ud83d\udcbe Saving Best Model Pipeline...\")\n",
    "        \n",
    "        # Determine best model\n",
    "        best_score = 0\n",
    "        best_model_name = None\n",
    "        \n",
    "        for name, results in self.evaluation_results.items():\n",
    "            score = results['metrics'][selection_metric]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model_name = name\n",
    "        \n",
    "        print(f\"   \ud83c\udfc6 Best model: {best_model_name} ({selection_metric}: {best_score:.4f})\")\n",
    "        \n",
    "        # Prepare model package\n",
    "        model_package = {\n",
    "            'best_model_name': best_model_name,\n",
    "            'best_score': best_score,\n",
    "            'selection_metric': selection_metric,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'label_names': label_names,\n",
    "            'model_metadata': {\n",
    "                'drug_vocab_size': self.preprocessor.drug_vocab_size,\n",
    "                'feature_dim': self.preprocessor.feature_dim,\n",
    "                'max_drugs': self.preprocessor.max_drugs,\n",
    "                'training_samples': len(y_train),\n",
    "                'test_samples': len(y_test)\n",
    "            },\n",
    "            'performance_metrics': self.evaluation_results[best_model_name]['metrics'],\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        # Save model based on type\n",
    "        if best_model_name == \"Random Forest\":\n",
    "            model_package['model'] = rf_classifier.model\n",
    "            model_package['feature_selector'] = rf_classifier.feature_selector\n",
    "            model_package['best_params'] = rf_classifier.best_params\n",
    "            model_filename = 'best_random_forest_model.pkl'\n",
    "            \n",
    "        elif best_model_name == \"XGBoost\":\n",
    "            model_package['model'] = xgb_classifier.model\n",
    "            model_package['best_params'] = xgb_classifier.best_params\n",
    "            model_filename = 'best_xgboost_model.pkl'\n",
    "            \n",
    "        elif best_model_name == \"PyTorch Neural Network\":\n",
    "            # For PyTorch, save model state dict and architecture info\n",
    "            model_package['model_state_dict'] = trainer.model.state_dict()\n",
    "            model_package['model_architecture'] = {\n",
    "                'input_size': input_size,\n",
    "                'drug_vocab_size': preprocessor.drug_vocab_size,\n",
    "                'embedding_dim': 128,\n",
    "                'hidden_sizes': [512, 256, 128],\n",
    "                'num_classes': 2,\n",
    "                'dropout_rate': 0.3,\n",
    "                'max_drugs': 10\n",
    "            }\n",
    "            model_package['training_history'] = pytorch_results['training_history']\n",
    "            model_filename = 'best_pytorch_model.pkl'\n",
    "        \n",
    "        # Save the complete package\n",
    "        with open(model_filename, 'wb') as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        print(f\"   \u2713 Model package saved: {model_filename}\")\n",
    "        print(f\"   \ud83d\udcca Package size: {self._get_file_size(model_filename):.2f} MB\")\n",
    "        \n",
    "        # Save preprocessor separately for easy access\n",
    "        with open('drug_interaction_preprocessor.pkl', 'wb') as f:\n",
    "            pickle.dump(self.preprocessor, f)\n",
    "        print(f\"   \u2713 Preprocessor saved: drug_interaction_preprocessor.pkl\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        with open('model_evaluation_results.pkl', 'wb') as f:\n",
    "            pickle.dump(self.evaluation_results, f)\n",
    "        print(f\"   \u2713 Evaluation results saved: model_evaluation_results.pkl\")\n",
    "        \n",
    "        # Create summary report\n",
    "        self._create_summary_report(model_package, model_filename)\n",
    "        \n",
    "        return model_filename, model_package\n",
    "    \n",
    "    def _get_file_size(self, filename):\n",
    "        \"\"\"Get file size in MB\"\"\"\n",
    "        import os\n",
    "        return os.path.getsize(filename) / (1024 * 1024)\n",
    "    \n",
    "    def _create_summary_report(self, model_package, model_filename):\n",
    "        \"\"\"Create a detailed summary report\"\"\"\n",
    "        report_filename = 'model_summary_report.txt'\n",
    "        \n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(\"=\"*80 + \"\\\\n\")\n",
    "            f.write(\"DRUG INTERACTION PREDICTION MODEL - SUMMARY REPORT\\\\n\")\n",
    "            f.write(\"=\"*80 + \"\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(f\"Generated on: {model_package['timestamp']}\\\\n\")\n",
    "            f.write(f\"Best Model: {model_package['best_model_name']}\\\\n\")\n",
    "            f.write(f\"Selection Metric: {model_package['selection_metric']}\\\\n\")\n",
    "            f.write(f\"Best Score: {model_package['best_score']:.4f}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"DATASET INFORMATION:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            metadata = model_package['model_metadata']\n",
    "            f.write(f\"Training Samples: {metadata['training_samples']:,}\\\\n\")\n",
    "            f.write(f\"Test Samples: {metadata['test_samples']:,}\\\\n\")\n",
    "            f.write(f\"Drug Vocabulary Size: {metadata['drug_vocab_size']:,}\\\\n\")\n",
    "            f.write(f\"Feature Dimensions: {metadata['feature_dim']:,}\\\\n\")\n",
    "            f.write(f\"Maximum Drugs per Prescription: {metadata['max_drugs']}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"PERFORMANCE METRICS:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            metrics = model_package['performance_metrics']\n",
    "            for metric_name, value in metrics.items():\n",
    "                if metric_name != 'confusion_matrix':\n",
    "                    f.write(f\"{metric_name.capitalize().replace('_', ' ')}: {value:.4f}\\\\n\")\n",
    "            \n",
    "            f.write(\"\\\\nCONFUSION MATRIX:\\\\n\")\n",
    "            cm = metrics['confusion_matrix']\n",
    "            f.write(f\"{'':>12} {'Pred Safe':>12} {'Pred Unsafe':>12}\\\\n\")\n",
    "            f.write(f\"{'True Safe':>12} {cm[0][0]:>12} {cm[0][1]:>12}\\\\n\")\n",
    "            f.write(f\"{'True Unsafe':>12} {cm[1][0]:>12} {cm[1][1]:>12}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"FILES CREATED:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            f.write(f\"Model Package: {model_filename}\\\\n\")\n",
    "            f.write(f\"Preprocessor: drug_interaction_preprocessor.pkl\\\\n\")\n",
    "            f.write(f\"Evaluation Results: model_evaluation_results.pkl\\\\n\")\n",
    "            f.write(f\"Summary Report: {report_filename}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"USAGE INSTRUCTIONS:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            f.write(\"1. Load the preprocessor: pickle.load(open('drug_interaction_preprocessor.pkl', 'rb'))\\\\n\")\n",
    "            f.write(f\"2. Load the model package: pickle.load(open('{model_filename}', 'rb'))\\\\n\")\n",
    "            f.write(\"3. For new predictions, use the preprocessor to transform data\\\\n\")\n",
    "            f.write(\"4. Apply the loaded model to make predictions\\\\n\\\\n\")\n",
    "            \n",
    "            if model_package['best_model_name'] == \"PyTorch Neural Network\":\n",
    "                f.write(\"PYTORCH MODEL SPECIFIC INSTRUCTIONS:\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"1. Recreate model architecture using saved parameters\\\\n\")\n",
    "                f.write(\"2. Load state dict: model.load_state_dict(package['model_state_dict'])\\\\n\")\n",
    "                f.write(\"3. Set model to evaluation mode: model.eval()\\\\n\\\\n\")\n",
    "        \n",
    "        print(f\"   \ud83d\udccb Summary report created: {report_filename}\")\n",
    "\n",
    "# Create model persistence system\n",
    "model_dict = {\n",
    "    \"Random Forest\": rf_classifier,\n",
    "    \"XGBoost\": xgb_classifier, \n",
    "    \"PyTorch Neural Network\": trainer\n",
    "}\n",
    "\n",
    "persistence_manager = ModelPersistence(evaluation_results, model_dict, preprocessor)\n",
    "best_model_file, saved_model_package = persistence_manager.save_best_model(selection_metric='roc_auc')\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe Model Persistence Summary:\")\n",
    "print(f\"   Best Model File: {best_model_file}\")\n",
    "print(f\"   Best Model: {saved_model_package['best_model_name']}\")\n",
    "print(f\"   Performance: {saved_model_package['best_score']:.4f} ROC-AUC\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced_predictor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Enhanced Interactive Drug Combination Prediction with Parallel Processing\n",
    "\n",
    "class EnhancedDrugCombinationPredictor:\n",
    "    \"\"\"\n",
    "    Enhanced drug combination safety predictor with parallel combination checking.\n",
    "    Utilizes CUDA kernel for efficient parallel processing of all drug combinations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, best_model_package, preprocessor, cuda_kernel=None):\n",
    "        self.model_package = best_model_package\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model_name = best_model_package['best_model_name']\n",
    "        self.cuda_kernel = cuda_kernel or cuda_combination_kernel\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the appropriate model based on type\"\"\"\n",
    "        if self.model_name == \"Random Forest\":\n",
    "            self.model = self.model_package['model']\n",
    "            self.feature_selector = self.model_package.get('feature_selector')\n",
    "            \n",
    "        elif self.model_name == \"XGBoost\":\n",
    "            self.model = self.model_package['model']\n",
    "            \n",
    "        elif self.model_name == \"PyTorch Neural Network\":\n",
    "            # Recreate PyTorch model\n",
    "            arch = self.model_package['model_architecture']\n",
    "            self.model = AdvancedDrugInteractionNet(\n",
    "                input_size=arch['input_size'],\n",
    "                drug_vocab_size=arch['drug_vocab_size'],\n",
    "                embedding_dim=arch['embedding_dim'],\n",
    "                hidden_sizes=arch['hidden_sizes'],\n",
    "                num_classes=arch['num_classes'],\n",
    "                dropout_rate=arch['dropout_rate'],\n",
    "                max_drugs=arch['max_drugs']\n",
    "            )\n",
    "            self.model.load_state_dict(self.model_package['model_state_dict'])\n",
    "            self.model.to(device)\n",
    "            self.model.eval()\n",
    "    \n",
    "    def predict_single_combination(self, drugs, dosage=None, return_confidence=True):\n",
    "        \"\"\"\n",
    "        Predict safety for a single drug combination.\n",
    "        \n",
    "        Args:\n",
    "            drugs (list): List of drug names\n",
    "            dosage (float, optional): Dosage per 24 hours\n",
    "            return_confidence (bool): Whether to return confidence scores\n",
    "            \n",
    "        Returns:\n",
    "            dict: Prediction results with safety label and confidence\n",
    "        \"\"\"\n",
    "        if len(drugs) < 2:\n",
    "            return {\"error\": \"At least 2 drugs required for interaction prediction\"}\n",
    "        \n",
    "        import pandas as pd\n",
    "        \n",
    "        # Create prediction DataFrame\n",
    "        prediction_data = {}\n",
    "        \n",
    "        # Fill drug columns\n",
    "        for i in range(1, 11):  # max_drugs = 10\n",
    "            col_name = f'drug{i}'\n",
    "            if i <= len(drugs):\n",
    "                prediction_data[col_name] = [drugs[i-1]]\n",
    "            else:\n",
    "                prediction_data[col_name] = [None]\n",
    "        \n",
    "        # Add other features\n",
    "        prediction_data.update({\n",
    "            'doses_per_24_hrs': [dosage if dosage is not None else 0.0],\n",
    "            'total_drugs': [len(drugs)],\n",
    "            'has_dosage_info': [1 if dosage is not None else 0],\n",
    "            'subject_id': [0],  # Dummy value\n",
    "            'drug_combination_id': ['_'.join(drugs)],\n",
    "            'safety_label': ['unknown']  # Placeholder\n",
    "        })\n",
    "        \n",
    "        df_pred = pd.DataFrame(prediction_data)\n",
    "        \n",
    "        # Transform using preprocessor\n",
    "        processed_data = self.preprocessor.transform(df_pred)\n",
    "        \n",
    "        # Make prediction based on model type\n",
    "        if self.model_name == \"Random Forest\":\n",
    "            X_pred = processed_data['sklearn']\n",
    "            if self.feature_selector:\n",
    "                X_pred = self.feature_selector.transform(X_pred)\n",
    "            \n",
    "            prediction = self.model.predict(X_pred)[0]\n",
    "            if return_confidence:\n",
    "                probabilities = self.model.predict_proba(X_pred)[0]\n",
    "                confidence = max(probabilities)\n",
    "                safe_prob = probabilities[0]\n",
    "                unsafe_prob = probabilities[1]\n",
    "            \n",
    "        elif self.model_name == \"XGBoost\":\n",
    "            import xgboost as xgb\n",
    "            X_pred = processed_data['sklearn']\n",
    "            dtest = xgb.DMatrix(X_pred)\n",
    "            \n",
    "            probability = self.model.predict(dtest)[0]\n",
    "            prediction = int(probability > 0.5)\n",
    "            \n",
    "            if return_confidence:\n",
    "                safe_prob = 1 - probability\n",
    "                unsafe_prob = probability\n",
    "                confidence = max(safe_prob, unsafe_prob)\n",
    "                \n",
    "        elif self.model_name == \"PyTorch Neural Network\":\n",
    "            import torch.nn.functional as F\n",
    "            X_pred = processed_data['pytorch']\n",
    "            X_tensor = torch.FloatTensor(X_pred).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(X_tensor)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                prediction = output.argmax(dim=1).item()\n",
    "                \n",
    "                if return_confidence:\n",
    "                    safe_prob = probs[0][0].item()\n",
    "                    unsafe_prob = probs[0][1].item()\n",
    "                    confidence = max(safe_prob, unsafe_prob)\n",
    "        \n",
    "        # Convert prediction to label\n",
    "        safety_label = 'safe' if prediction == 0 else 'unsafe'\n",
    "        \n",
    "        result = {\n",
    "            \"drugs\": drugs,\n",
    "            \"dosage\": f\"{dosage} per 24hrs\" if dosage else \"Not specified\",\n",
    "            \"prediction\": safety_label,\n",
    "            \"model_used\": self.model_name\n",
    "        }\n",
    "        \n",
    "        if return_confidence:\n",
    "            result.update({\n",
    "                \"confidence\": confidence,\n",
    "                \"safe_probability\": safe_prob,\n",
    "                \"unsafe_probability\": unsafe_prob\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_all_combinations(self, drugs, dosage=None):\n",
    "        \"\"\"\n",
    "        Check ALL combinations of the given drugs in parallel using CUDA kernel.\n",
    "        For example, if 5 drugs are given, checks all 2-drug, 3-drug, 4-drug, and 5-drug\n",
    "        combinations in parallel on GPU.\n",
    "        \n",
    "        Args:\n",
    "            drugs (list): List of 2-10 drug names\n",
    "            dosage (float, optional): Dosage information if available\n",
    "        \n",
    "        Returns:\n",
    "            dict: Comprehensive results for all combinations\n",
    "        \"\"\"\n",
    "        if len(drugs) < 2:\n",
    "            return {\"error\": \"At least 2 drugs required\"}\n",
    "        \n",
    "        if len(drugs) > 10:\n",
    "            print(\"\u26a0\ufe0f More than 10 drugs provided, using first 10 only\")\n",
    "            drugs = drugs[:10]\n",
    "        \n",
    "        print(f\"\\n\ud83d\ude80 Analyzing all combinations of {len(drugs)} drugs in parallel...\")\n",
    "        \n",
    "        # Use CUDA kernel for parallel checking\n",
    "        results = self.cuda_kernel.parallel_combination_check(\n",
    "            drugs, self.model, self.preprocessor, dosage\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_multiple_combinations(self, drug_combinations, dosages=None):\n",
    "        \"\"\"Analyze multiple specific drug combinations at once\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, drugs in enumerate(drug_combinations):\n",
    "            dosage = dosages[i] if dosages and i < len(dosages) else None\n",
    "            result = self.predict_single_combination(drugs, dosage)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_predictions(self, prediction_results):\n",
    "        \"\"\"Create visualization for prediction results\"\"\"\n",
    "        if not prediction_results or 'results' not in prediction_results:\n",
    "            return\n",
    "        \n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        results = prediction_results['results']\n",
    "        \n",
    "        # Extract data\n",
    "        combo_sizes = [len(r['drugs']) for r in results]\n",
    "        predictions = [r['prediction'] for r in results]\n",
    "        confidences = [r['confidence'] for r in results]\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Plot 1: Prediction distribution\n",
    "        safe_count = predictions.count('safe')\n",
    "        unsafe_count = predictions.count('unsafe')\n",
    "        axes[0].pie([safe_count, unsafe_count], labels=['Safe', 'Unsafe'], \n",
    "                    autopct='%1.1f%%', colors=['green', 'red'])\n",
    "        axes[0].set_title('Overall Safety Distribution')\n",
    "        \n",
    "        # Plot 2: Combinations by size\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame({'size': combo_sizes, 'prediction': predictions})\n",
    "        size_counts = df.groupby(['size', 'prediction']).size().unstack(fill_value=0)\n",
    "        size_counts.plot(kind='bar', ax=axes[1], color=['green', 'red'])\n",
    "        axes[1].set_title('Safety by Combination Size')\n",
    "        axes[1].set_xlabel('Number of Drugs')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].legend(['Safe', 'Unsafe'])\n",
    "        \n",
    "        # Plot 3: Confidence distribution\n",
    "        axes[2].hist(confidences, bins=20, edgecolor='black')\n",
    "        axes[2].set_title('Prediction Confidence Distribution')\n",
    "        axes[2].set_xlabel('Confidence')\n",
    "        axes[2].set_ylabel('Count')\n",
    "        axes[2].axvline(np.mean(confidences), color='red', linestyle='--', \n",
    "                        label=f'Mean: {np.mean(confidences):.3f}')\n",
    "        axes[2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\u2713 Enhanced Drug Combination Predictor created!\")\n",
    "print(\"  Features:\")\n",
    "print(\"    - Single combination prediction\")\n",
    "print(\"    - Parallel checking of ALL combinations (2 to N drugs)\")\n",
    "print(\"    - CUDA-accelerated inference\")\n",
    "print(\"    - Conditional dosage handling\")\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Demonstration of Enhanced Features\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE DEMONSTRATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create enhanced predictor\n",
    "enhanced_predictor = EnhancedDrugCombinationPredictor(\n",
    "    best_model_package=saved_model_package,\n",
    "    preprocessor=preprocessor,\n",
    "    cuda_kernel=cuda_combination_kernel\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Enhanced predictor initialized\")\n",
    "print(f\"  Model: {enhanced_predictor.model_name}\")\n",
    "print(f\"  Using CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Example 1: Single combination prediction\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 1: Single Drug Combination\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_drugs_1 = ['Aspirin', 'Ibuprofen']\n",
    "result_1 = enhanced_predictor.predict_single_combination(test_drugs_1, dosage=100.0)\n",
    "\n",
    "print(f\"\\nDrugs: {result_1['drugs']}\")\n",
    "print(f\"Dosage: {result_1['dosage']}\")\n",
    "print(f\"Prediction: {result_1['prediction']}\")\n",
    "print(f\"Confidence: {result_1.get('confidence', 0):.2%}\")\n",
    "print(f\"Safe probability: {result_1.get('safe_probability', 0):.2%}\")\n",
    "print(f\"Unsafe probability: {result_1.get('unsafe_probability', 0):.2%}\")\n",
    "\n",
    "# Example 2: Parallel checking of all combinations (the key feature!)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 2: Parallel Checking of ALL Combinations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_drugs_2 = ['Aspirin', 'Warfarin', 'Ibuprofen', 'Naproxen', 'Clopidogrel']\n",
    "print(f\"\\nInput: {len(test_drugs_2)} drugs: {test_drugs_2}\")\n",
    "\n",
    "# This will check ALL combinations in parallel:\n",
    "# - All 2-drug combinations (C(5,2) = 10)\n",
    "# - All 3-drug combinations (C(5,3) = 10)\n",
    "# - All 4-drug combinations (C(5,4) = 5)\n",
    "# - All 5-drug combinations (C(5,5) = 1)\n",
    "# Total: 26 combinations checked in parallel on GPU!\n",
    "\n",
    "all_combo_results = enhanced_predictor.predict_all_combinations(test_drugs_2, dosage=150.0)\n",
    "\n",
    "print(f\"\\nTotal combinations checked: {all_combo_results['total_combinations']}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Safe combinations: {all_combo_results['summary']['safe_combinations']}\")\n",
    "print(f\"  Unsafe combinations: {all_combo_results['summary']['unsafe_combinations']}\")\n",
    "print(f\"  Safety percentage: {all_combo_results['summary']['safety_percentage']:.1f}%\")\n",
    "\n",
    "print(f\"\\nFirst 10 results:\")\n",
    "for i, result in enumerate(all_combo_results['results'][:10]):\n",
    "    drugs_str = ' + '.join(result['drugs'])\n",
    "    print(f\"  {i+1}. [{len(result['drugs'])} drugs] {drugs_str}: {result['prediction'].upper()} \"\n",
    "          f\"(confidence: {result['confidence']:.2%})\")\n",
    "\n",
    "# Visualize results\n",
    "print(f\"\\nVisualizing all combination results...\")\n",
    "enhanced_predictor.visualize_predictions(all_combo_results)\n",
    "\n",
    "# Example 3: Incremental Learning\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 3: Incremental Learning (PyTorch model only)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if enhanced_predictor.model_name == \"PyTorch Neural Network\":\n",
    "    # Create incremental learner\n",
    "    learner = IncrementalLearner(enhanced_predictor.model, preprocessor, device=device)\n",
    "    \n",
    "    # Simulate new drug combinations discovered after deployment\n",
    "    new_combinations = [\n",
    "        ['Aspirin', 'NewDrug1'],\n",
    "        ['Warfarin', 'NewDrug2'],\n",
    "        ['Ibuprofen', 'NewDrug3']\n",
    "    ]\n",
    "    new_labels = [1, 1, 0]  # 1=unsafe, 0=safe\n",
    "    new_dosages = [100.0, 150.0, 200.0]\n",
    "    \n",
    "    print(f\"\\nLearning from {len(new_combinations)} new drug combinations...\")\n",
    "    \n",
    "    learning_result = learner.learn_from_new_data(\n",
    "        new_combinations, new_labels, new_dosages, \n",
    "        learning_rate=0.0001, epochs=5\n",
    "    )\n",
    "    \n",
    "    if learning_result:\n",
    "        print(f\"\\n\u2713 Incremental learning completed!\")\n",
    "        print(f\"  Samples learned: {learning_result['num_samples_learned']}\")\n",
    "        print(f\"  Final loss: {learning_result['final_loss']:.4f}\")\n",
    "        \n",
    "        # Save updated model\n",
    "        learner.save_updated_model('incremental_model_updated.pth')\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f Incremental learning only available for PyTorch model\")\n",
    "    print(f\"  Current model: {enhanced_predictor.model_name}\")\n",
    "\n",
    "# Example 4: Dosage handling\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE 4: Dosage Handling (Optional)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_drugs_4 = ['Aspirin', 'Warfarin']\n",
    "\n",
    "# With dosage\n",
    "result_with_dosage = enhanced_predictor.predict_single_combination(test_drugs_4, dosage=100.0)\n",
    "print(f\"\\nWith dosage (100.0):\")\n",
    "print(f\"  Prediction: {result_with_dosage['prediction']}\")\n",
    "print(f\"  Confidence: {result_with_dosage.get('confidence', 0):.2%}\")\n",
    "\n",
    "# Without dosage\n",
    "result_no_dosage = enhanced_predictor.predict_single_combination(test_drugs_4, dosage=None)\n",
    "print(f\"\\nWithout dosage:\")\n",
    "print(f\"  Prediction: {result_no_dosage['prediction']}\")\n",
    "print(f\"  Confidence: {result_no_dosage.get('confidence', 0):.2%}\")\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f Note: Model handles missing dosage through 'has_dosage_info' feature\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF ENHANCEMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\u2713 Custom CUDA kernel for parallel combination checking\")\n",
    "print(\"\u2713 All k-way combinations (k=2 to N) checked in parallel on GPU\")\n",
    "print(\"\u2713 Training considers all combinations within each row\")\n",
    "print(\"\u2713 Conditional dosage handling (works with or without dosage)\")\n",
    "print(\"\u2713 Incremental learning capability for continuous improvement\")\n",
    "print(\"\u2713 Three models trained: Random Forest, XGBoost, PyTorch Neural Network\")\n",
    "print(\"\u2713 Best model automatically selected and saved\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73faf3",
   "metadata": {},
   "source": [
    "# Final Summary and Conclusions\n",
    "\n",
    "## \ud83c\udfc6 Multi-Model Comparison Results\n",
    "\n",
    "This notebook successfully implemented and compared three different machine learning approaches for drug interaction safety prediction:\n",
    "\n",
    "### Models Implemented:\n",
    "1. **Random Forest Classifier** - Ensemble method with CUDA-accelerated feature selection\n",
    "2. **XGBoost Classifier** - Gradient boosting with native GPU acceleration  \n",
    "3. **PyTorch Neural Network** - Deep learning with drug embeddings and attention mechanisms\n",
    "\n",
    "### Key Technical Achievements:\n",
    "\n",
    "#### \u26a1 CUDA Optimization:\n",
    "- Custom CUDA memory management and optimization\n",
    "- GPU-accelerated XGBoost training (`gpu_hist` tree method)\n",
    "- PyTorch model with CUDA acceleration and optimized batch processing\n",
    "- Memory-efficient data loading with proper device management\n",
    "\n",
    "#### \ud83e\udde0 Advanced Neural Architecture:\n",
    "- Drug embedding layers for better representation learning\n",
    "- Multi-head attention mechanism for drug interaction modeling\n",
    "- Batch normalization and dropout for regularization\n",
    "- Residual connections where applicable\n",
    "\n",
    "#### \ud83d\udcca Comprehensive Evaluation:\n",
    "- ROC curves and Precision-Recall analysis\n",
    "- Confusion matrices and performance metrics\n",
    "- Feature importance analysis for tree-based models\n",
    "- Training history visualization for neural networks\n",
    "\n",
    "#### \ud83d\udcbe Production-Ready Persistence:\n",
    "- Best model automatically selected based on ROC-AUC\n",
    "- Complete model pipeline saved as PKL file\n",
    "- Preprocessor and metadata preservation\n",
    "- Detailed summary reports for deployment\n",
    "\n",
    "### Dataset Characteristics:\n",
    "- **Source**: Combined dataset from Scala preprocessing (CombineDatasets.scala)\n",
    "- **Features**: Up to 10 drugs per prescription + dosage information\n",
    "- **Labels**: Binary classification (safe/unsafe combinations)\n",
    "- **Preprocessing**: Advanced feature engineering with drug embeddings and numerical features\n",
    "\n",
    "### Model Performance Comparison:\n",
    "The notebook automatically identifies and saves the best performing model based on ROC-AUC score, ensuring optimal performance for production deployment.\n",
    "\n",
    "### Files Generated:\n",
    "- `best_[model_type]_model.pkl` - Complete model package\n",
    "- `drug_interaction_preprocessor.pkl` - Preprocessor for new predictions\n",
    "- `model_evaluation_results.pkl` - Comprehensive evaluation results\n",
    "- `model_summary_report.txt` - Detailed performance report\n",
    "- Various visualization plots (ROC curves, confusion matrices, etc.)\n",
    "\n",
    "### Usage in Production:\n",
    "The saved model can be loaded and used for real-time drug interaction prediction in clinical decision support systems, pharmacy management software, or prescription validation tools.\n",
    "\n",
    "### Next Steps:\n",
    "1. **Model Deployment**: Deploy best model as REST API or web service\n",
    "2. **Real-time Integration**: Integrate with pharmacy/hospital management systems  \n",
    "3. **Continuous Learning**: Implement feedback loop for model updates\n",
    "4. **Explainability**: Add SHAP or LIME for prediction explanations\n",
    "5. **Multi-class Extension**: Extend to predict interaction severity levels\n",
    "\n",
    "The comprehensive approach ensures robust, production-ready drug interaction prediction with optimal performance and thorough evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}