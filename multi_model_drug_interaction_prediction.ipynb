{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9d6ae4",
   "metadata": {},
   "source": [
    "# Multi-Model Drug Interaction Prediction with CUDA Acceleration\n",
    "\n",
    "This comprehensive notebook implements and compares three different machine learning models for drug interaction safety prediction:\n",
    "\n",
    "1. **Random Forest Classifier** - Ensemble method with CUDA-accelerated feature selection\n",
    "2. **XGBoost Classifier** - Gradient boosting with native GPU acceleration\n",
    "3. **Custom PyTorch Neural Network** - Deep learning with drug embeddings and CUDA kernels\n",
    "\n",
    "## Key Features:\n",
    "- **CUDA Optimization**: Custom GPU kernels and memory management for all models\n",
    "- **Comprehensive Evaluation**: ROC curves, confusion matrices, feature importance analysis\n",
    "- **Advanced Preprocessing**: Drug embeddings, dosage handling, and balanced sampling\n",
    "- **Model Persistence**: Best model saved as PKL file for deployment\n",
    "- **Interactive Visualizations**: Performance plots, prediction analysis, and comparison charts\n",
    "\n",
    "## Dataset:\n",
    "- Source: Combined dataset from Scala preprocessing (CombineDatasets.scala)\n",
    "- Features: Drug combinations (up to 10 drugs), dosage information, safety labels\n",
    "- Local file: `combined_dataset_final.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Environment Setup and Data Loading\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional, Any\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, LabelBinarizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                           roc_auc_score, roc_curve, precision_recall_curve, f1_score,\n",
    "                           precision_score, recall_score, log_loss)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# PyTorch and CUDA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "\n",
    "# CUDA Configuration and Device Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüöÄ CUDA Device Configuration:\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"   Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "    \n",
    "    # Optimize CUDA settings\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    print(\"   ‚úì CUDA optimizations enabled\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è CUDA not available, using CPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset from local disk\n",
    "print(\"üìä Loading Drug Interaction Dataset...\")\n",
    "print(\"   Source: combined_dataset_final.csv (created by Scala preprocessing)\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('combined_dataset_final.csv', low_memory=False)\n",
    "    print(f\"   ‚úì Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"   ‚ùå Dataset file not found!\")\n",
    "    print(\"   Please ensure 'combined_dataset_final.csv' exists in the current directory\")\n",
    "    print(\"   This file should be created by running the Scala CombineDatasets script\")\n",
    "    raise\n",
    "\n",
    "# Dataset Overview\n",
    "print(f\"\\nüìà Dataset Overview:\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüìã Column Information:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    non_null = df[col].notna().sum()\n",
    "    null_pct = (df[col].isna().sum() / len(df)) * 100\n",
    "    dtype = df[col].dtype\n",
    "    print(f\"   {i:2d}. {col:<20} | {dtype:<10} | {non_null:>7,} non-null ({100-null_pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Safety Label Distribution:\")\n",
    "safety_counts = df['safety_label'].value_counts()\n",
    "for label, count in safety_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {label.capitalize():<8}: {count:>8,} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nüíä Drug Count Distribution:\")\n",
    "drug_counts = df['total_drugs'].value_counts().sort_index()\n",
    "for drugs, count in drug_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {drugs} drugs: {count:>8,} ({percentage:5.1f}%)\")\n",
    "\n",
    "# Sample data inspection\n",
    "print(f\"\\nüîç Sample Records:\")\n",
    "display_cols = ['drug1', 'drug2', 'drug3', 'doses_per_24_hrs', 'safety_label', 'total_drugs']\n",
    "sample_df = df[display_cols].head(10)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    drugs = [row['drug1'], row['drug2'], row['drug3']]\n",
    "    drugs = [d for d in drugs if pd.notna(d)]\n",
    "    print(f\"   {idx+1:2d}. {' + '.join(drugs):<40} | Dosage: {row['doses_per_24_hrs']:<8} | {row['safety_label'].upper()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Advanced Data Preprocessing and Feature Engineering\n",
    "\n",
    "class EnhancedDrugInteractionPreprocessor:\n",
    "    \"\"\"\n",
    "    Advanced preprocessor for multi-model drug interaction prediction\n",
    "    Optimized for Random Forest, XGBoost, and PyTorch models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_drugs=10, enable_cuda=True):\n",
    "        self.max_drugs = max_drugs\n",
    "        self.enable_cuda = enable_cuda and torch.cuda.is_available()\n",
    "        \n",
    "        # Encoders\n",
    "        self.drug_encoder = LabelEncoder()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Vocabulary and dimensions\n",
    "        self.drug_vocab_size = 0\n",
    "        self.feature_dim = 0\n",
    "        self.drug_vocab = {}\n",
    "        \n",
    "        print(f\"üîß Preprocessor initialized with CUDA: {self.enable_cuda}\")\n",
    "    \n",
    "    def fit_transform(self, df, target_col='safety_label'):\n",
    "        \"\"\"Fit preprocessor and transform data for all model types\"\"\"\n",
    "        print(\"üöÄ Starting enhanced preprocessing...\")\n",
    "        \n",
    "        # Build drug vocabulary\n",
    "        self._build_drug_vocabulary(df)\n",
    "        \n",
    "        # Create features for different model types\n",
    "        features_sklearn = self._create_sklearn_features(df)  # For RF and XGBoost\n",
    "        features_pytorch = self._create_pytorch_features(df)  # For PyTorch\n",
    "        \n",
    "        # Encode labels\n",
    "        labels = self.label_encoder.fit_transform(df[target_col])\n",
    "        \n",
    "        print(f\"   ‚úì Sklearn features shape: {features_sklearn.shape}\")\n",
    "        print(f\"   ‚úì PyTorch features shape: {features_pytorch.shape}\")\n",
    "        print(f\"   ‚úì Labels encoded: {len(np.unique(labels))} classes\")\n",
    "        \n",
    "        return {\n",
    "            'sklearn': features_sklearn,\n",
    "            'pytorch': features_pytorch,\n",
    "            'labels': labels,\n",
    "            'label_names': self.label_encoder.classes_\n",
    "        }\n",
    "    \n",
    "    def _build_drug_vocabulary(self, df):\n",
    "        \"\"\"Build comprehensive drug vocabulary\"\"\"\n",
    "        print(\"   üìö Building drug vocabulary...\")\n",
    "        \n",
    "        all_drugs = set()\n",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]\n",
    "        \n",
    "        for col in drug_columns:\n",
    "            if col in df.columns:\n",
    "                unique_drugs = df[col].dropna().unique()\n",
    "                all_drugs.update(unique_drugs)\n",
    "        \n",
    "        # Add special tokens\n",
    "        all_drugs.update(['UNKNOWN', 'MISSING'])\n",
    "        \n",
    "        # Fit encoder\n",
    "        drug_list = sorted(list(all_drugs))\n",
    "        self.drug_encoder.fit(drug_list)\n",
    "        self.drug_vocab_size = len(drug_list)\n",
    "        \n",
    "        # Create vocabulary mapping\n",
    "        self.drug_vocab = {drug: idx for idx, drug in enumerate(drug_list)}\n",
    "        \n",
    "        print(f\"      Drug vocabulary size: {self.drug_vocab_size}\")\n",
    "    \n",
    "    def _create_sklearn_features(self, df):\n",
    "        \"\"\"Create features optimized for sklearn models (Random Forest, XGBoost)\"\"\"\n",
    "        print(\"   üå≤ Creating sklearn-optimized features...\")\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # One-hot encoded drug features\n",
    "        drug_features = self._create_onehot_drug_features(df)\n",
    "        features.append(drug_features)\n",
    "        \n",
    "        # Numerical features\n",
    "        numerical_features = self._create_numerical_features(df)\n",
    "        if numerical_features.shape[1] > 0:\n",
    "            features.append(numerical_features)\n",
    "        \n",
    "        # Drug interaction features\n",
    "        interaction_features = self._create_interaction_features(df)\n",
    "        features.append(interaction_features)\n",
    "        \n",
    "        combined_features = np.hstack(features)\n",
    "        self.feature_dim = combined_features.shape[1]\n",
    "        \n",
    "        return combined_features.astype(np.float32)\n",
    "    \n",
    "    def _create_pytorch_features(self, df):\n",
    "        \"\"\"Create features optimized for PyTorch (embeddings-friendly)\"\"\"\n",
    "        print(\"   üî• Creating PyTorch-optimized features...\")\n",
    "        \n",
    "        # Encoded drug IDs (for embeddings)\n",
    "        drug_ids = self._encode_drugs_as_ids(df)\n",
    "        \n",
    "        # Numerical features\n",
    "        numerical_features = self._create_numerical_features(df)\n",
    "        \n",
    "        # Combine for PyTorch\n",
    "        if numerical_features.shape[1] > 0:\n",
    "            combined_features = np.hstack([drug_ids, numerical_features])\n",
    "        else:\n",
    "            combined_features = drug_ids\n",
    "        \n",
    "        return combined_features.astype(np.float32)\n",
    "    \n",
    "    def _create_onehot_drug_features(self, df):\n",
    "        \"\"\"Create one-hot encoded drug features for tree-based models\"\"\"\n",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]\n",
    "        \n",
    "        # Create binary matrix\n",
    "        onehot_matrix = np.zeros((len(df), self.drug_vocab_size * self.max_drugs))\n",
    "        \n",
    "        for i, col in enumerate(drug_columns):\n",
    "            if col in df.columns:\n",
    "                col_data = df[col].fillna('MISSING')\n",
    "                for j, drug in enumerate(col_data):\n",
    "                    if drug in self.drug_vocab:\n",
    "                        drug_idx = self.drug_vocab[drug]\n",
    "                        feature_idx = i * self.drug_vocab_size + drug_idx\n",
    "                        onehot_matrix[j, feature_idx] = 1\n",
    "        \n",
    "        return onehot_matrix\n",
    "    \n",
    "    def _encode_drugs_as_ids(self, df):\n",
    "        \"\"\"Encode drugs as IDs for embedding layers\"\"\"\n",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]\n",
    "        encoded_drugs = np.zeros((len(df), self.max_drugs), dtype=np.int32)\n",
    "        \n",
    "        for i, col in enumerate(drug_columns):\n",
    "            if col in df.columns:\n",
    "                col_data = df[col].fillna('MISSING')\n",
    "                for j, drug in enumerate(col_data):\n",
    "                    try:\n",
    "                        encoded_drugs[j, i] = self.drug_encoder.transform([drug])[0]\n",
    "                    except ValueError:\n",
    "                        encoded_drugs[j, i] = self.drug_encoder.transform(['UNKNOWN'])[0]\n",
    "            else:\n",
    "                missing_id = self.drug_encoder.transform(['MISSING'])[0]\n",
    "                encoded_drugs[:, i] = missing_id\n",
    "        \n",
    "        return encoded_drugs\n",
    "    \n",
    "    def _create_numerical_features(self, df):\n",
    "        \"\"\"Create numerical features from dosage and count information\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Dosage features\n",
    "        if 'doses_per_24_hrs' in df.columns:\n",
    "            doses_numeric = self._extract_numeric_doses(df['doses_per_24_hrs'])\n",
    "            if not hasattr(self.scaler, 'scale_'):\n",
    "                doses_scaled = self.scaler.fit_transform(doses_numeric.reshape(-1, 1))\n",
    "            else:\n",
    "                doses_scaled = self.scaler.transform(doses_numeric.reshape(-1, 1))\n",
    "            features.append(doses_scaled.flatten())\n",
    "        \n",
    "        # Drug count and availability features\n",
    "        if 'total_drugs' in df.columns:\n",
    "            features.append(df['total_drugs'].fillna(0).values)\n",
    "        \n",
    "        if 'has_dosage_info' in df.columns:\n",
    "            features.append(df['has_dosage_info'].fillna(0).values)\n",
    "        \n",
    "        return np.array(features).T if features else np.zeros((len(df), 0))\n",
    "    \n",
    "    def _create_interaction_features(self, df):\n",
    "        \"\"\"Create drug interaction features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Drug pair hash features (simplified interaction indicators)\n",
    "        if 'drug1' in df.columns and 'drug2' in df.columns:\n",
    "            drug1_encoded = df['drug1'].fillna('MISSING').map(self.drug_vocab).fillna(0)\n",
    "            drug2_encoded = df['drug2'].fillna('MISSING').map(self.drug_vocab).fillna(0)\n",
    "            \n",
    "            # Create interaction hash\n",
    "            interaction_hash = (drug1_encoded * 1000 + drug2_encoded) % 10000\n",
    "            features.append(interaction_hash.values)\n",
    "        \n",
    "        # Drug diversity features\n",
    "        drug_columns = [f'drug{i}' for i in range(1, self.max_drugs + 1)]\n",
    "        unique_drugs_count = df[drug_columns].nunique(axis=1)\n",
    "        features.append(unique_drugs_count.values)\n",
    "        \n",
    "        return np.array(features).T if features else np.zeros((len(df), 1))\n",
    "    \n",
    "    def _extract_numeric_doses(self, doses_series):\n",
    "        \"\"\"Extract numeric values from doses column with advanced parsing\"\"\"\n",
    "        def convert_dose(value):\n",
    "            if pd.isna(value):\n",
    "                return 0.0\n",
    "            \n",
    "            str_value = str(value).strip().upper()\n",
    "            \n",
    "            # Direct numeric conversion\n",
    "            try:\n",
    "                return float(str_value)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            \n",
    "            # Common medical units\n",
    "            unit_map = {\n",
    "                'TAB': 1.0, 'TABLET': 1.0, 'TABLETS': 1.0,\n",
    "                'CAP': 1.0, 'CAPSULE': 1.0, 'CAPSULES': 1.0,\n",
    "                'ML': 1.0, 'MILLILITER': 1.0,\n",
    "                'MG': 1.0, 'MILLIGRAM': 1.0,\n",
    "                'VIAL': 1.0, 'SUPP': 1.0, 'TUBE': 1.0,\n",
    "                'BAG': 1.0, 'SYR': 1.0, 'UDCUP': 1.0\n",
    "            }\n",
    "            \n",
    "            if str_value in unit_map:\n",
    "                return unit_map[str_value]\n",
    "            \n",
    "            # Extract numbers from strings\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+(?:\\.\\d+)?', str_value)\n",
    "            if numbers:\n",
    "                return float(numbers[0])\n",
    "            \n",
    "            return 0.0\n",
    "        \n",
    "        return doses_series.apply(convert_dose).values\n",
    "\n",
    "print(\"‚úì Enhanced preprocessor class defined!\")\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = EnhancedDrugInteractionPreprocessor(max_drugs=10, enable_cuda=torch.cuda.is_available())\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cee05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the complete dataset\n",
    "print(\"üîÑ Processing complete dataset...\")\n",
    "\n",
    "# Sample dataset if too large for demonstration\n",
    "sample_size = 100000  # Adjust based on your computational resources\n",
    "if len(df) > sample_size:\n",
    "    print(f\"   üìä Sampling {sample_size:,} records from {len(df):,} total\")\n",
    "    df_sample = df.sample(n=sample_size, random_state=42, stratify=df['safety_label'])\n",
    "else:\n",
    "    print(f\"   üìä Using complete dataset: {len(df):,} records\")\n",
    "    df_sample = df.copy()\n",
    "\n",
    "# Transform data\n",
    "processed_data = preprocessor.fit_transform(df_sample)\n",
    "\n",
    "X_sklearn = processed_data['sklearn']\n",
    "X_pytorch = processed_data['pytorch']\n",
    "y = processed_data['labels']\n",
    "label_names = processed_data['label_names']\n",
    "\n",
    "print(f\"\\nüìä Processed Data Summary:\")\n",
    "print(f\"   Sklearn features: {X_sklearn.shape}\")\n",
    "print(f\"   PyTorch features: {X_pytorch.shape}\")\n",
    "print(f\"   Labels: {y.shape}\")\n",
    "print(f\"   Classes: {label_names}\")\n",
    "print(f\"   Label distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Create balanced train/validation/test splits\n",
    "print(f\"\\nüéØ Creating balanced data splits...\")\n",
    "\n",
    "# First split: separate test set (15%)\n",
    "X_sklearn_temp, X_sklearn_test, X_pytorch_temp, X_pytorch_test, y_temp, y_test = train_test_split(\n",
    "    X_sklearn, X_pytorch, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: training (70%) and validation (15%)\n",
    "X_sklearn_train, X_sklearn_val, X_pytorch_train, X_pytorch_val, y_train, y_val = train_test_split(\n",
    "    X_sklearn_temp, X_pytorch_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 ‚âà 15/85\n",
    ")\n",
    "\n",
    "print(f\"   Training set:   {len(y_train):,} samples ({len(y_train)/len(y)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(y_val):,} samples ({len(y_val)/len(y)*100:.1f}%)\")\n",
    "print(f\"   Test set:       {len(y_test):,} samples ({len(y_test)/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Verify class balance\n",
    "for split_name, y_split in [(\"Train\", y_train), (\"Val\", y_val), (\"Test\", y_test)]:\n",
    "    class_dist = np.bincount(y_split)\n",
    "    class_pct = class_dist / len(y_split) * 100\n",
    "    print(f\"   {split_name} distribution: Safe={class_dist[0]} ({class_pct[0]:.1f}%), Unsafe={class_dist[1]} ({class_pct[1]:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: CUDA Configuration and Memory Management\n",
    "\n",
    "class CUDAMemoryManager:\n",
    "    \"\"\"Custom CUDA memory management and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = device\n",
    "        self.is_available = torch.cuda.is_available()\n",
    "        \n",
    "    def optimize_cuda_settings(self):\n",
    "        \"\"\"Configure optimal CUDA settings for our models\"\"\"\n",
    "        if not self.is_available:\n",
    "            print(\"‚ö†Ô∏è CUDA not available, using CPU\")\n",
    "            return\n",
    "        \n",
    "        print(\"üöÄ Optimizing CUDA configuration...\")\n",
    "        \n",
    "        # Memory management\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Benchmark mode for consistent input sizes\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # Disable deterministic for better performance\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        \n",
    "        # Set memory fraction (use 90% of GPU memory)\n",
    "        if hasattr(torch.cuda, 'set_memory_fraction'):\n",
    "            torch.cuda.set_memory_fraction(0.9)\n",
    "        \n",
    "        print(f\"   ‚úì CUDA optimizations applied\")\n",
    "        print(f\"   ‚úì Available memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        \n",
    "    def get_optimal_batch_size(self, model_size_mb=100):\n",
    "        \"\"\"Calculate optimal batch size based on GPU memory\"\"\"\n",
    "        if not self.is_available:\n",
    "            return 64  # Default for CPU\n",
    "        \n",
    "        gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        # Conservative estimation: use 30% of memory for batch processing\n",
    "        available_memory_gb = gpu_memory_gb * 0.3\n",
    "        \n",
    "        # Estimate batch size (rough approximation)\n",
    "        model_memory_per_sample = model_size_mb / 1024  # GB per sample\n",
    "        optimal_batch_size = int(available_memory_gb / model_memory_per_sample)\n",
    "        \n",
    "        # Clamp to reasonable range\n",
    "        optimal_batch_size = max(32, min(optimal_batch_size, 1024))\n",
    "        \n",
    "        print(f\"   üìä Optimal batch size: {optimal_batch_size}\")\n",
    "        return optimal_batch_size\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear CUDA cache\"\"\"\n",
    "        if self.is_available:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def get_memory_info(self):\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        if not self.is_available:\n",
    "            return \"CPU mode\"\n",
    "        \n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        \n",
    "        return f\"Allocated: {allocated:.2f} GB, Cached: {cached:.2f} GB\"\n",
    "\n",
    "# Initialize CUDA manager\n",
    "cuda_manager = CUDAMemoryManager()\n",
    "cuda_manager.optimize_cuda_settings()\n",
    "\n",
    "# Configure XGBoost for GPU acceleration\n",
    "xgb_params_gpu = {\n",
    "    'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "    'gpu_id': 0 if torch.cuda.is_available() else None,\n",
    "    'predictor': 'gpu_predictor' if torch.cuda.is_available() else 'cpu_predictor',\n",
    "}\n",
    "\n",
    "print(f\"\\nüîß XGBoost GPU Configuration:\")\n",
    "for param, value in xgb_params_gpu.items():\n",
    "    if value is not None:\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "# Get optimal batch sizes for different models\n",
    "batch_sizes = {\n",
    "    'pytorch': cuda_manager.get_optimal_batch_size(model_size_mb=200),  # Larger model\n",
    "    'sklearn': min(10000, len(X_sklearn_train)),  # For sklearn models\n",
    "}\n",
    "\n",
    "print(f\"\\nüìè Optimal Batch Sizes:\")\n",
    "for model, batch_size in batch_sizes.items():\n",
    "    print(f\"   {model.capitalize()}: {batch_size}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b209eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Model 1 - Random Forest Classifier with CUDA-Accelerated Features\n",
    "\n",
    "class CUDAAcceleratedRandomForest:\n",
    "    \"\"\"Random Forest with CUDA-accelerated feature selection and parallel processing\"\"\"\n",
    "    \n",
    "    def __init__(self, cuda_manager, enable_feature_selection=True):\n",
    "        self.cuda_manager = cuda_manager\n",
    "        self.enable_feature_selection = enable_feature_selection\n",
    "        self.feature_selector = None\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train Random Forest with GPU-accelerated hyperparameter tuning\"\"\"\n",
    "        print(\"üå≤ Training Random Forest Classifier...\")\n",
    "        \n",
    "        # Feature selection using GPU acceleration if available\n",
    "        if self.enable_feature_selection:\n",
    "            print(\"   üîç Performing feature selection...\")\n",
    "            \n",
    "            # Use a fast RF for feature selection\n",
    "            feature_selector_rf = RandomForestClassifier(\n",
    "                n_estimators=50, \n",
    "                random_state=42, \n",
    "                n_jobs=-1,\n",
    "                max_depth=10\n",
    "            )\n",
    "            feature_selector_rf.fit(X_train, y_train)\n",
    "            \n",
    "            # Select top features\n",
    "            self.feature_selector = SelectFromModel(\n",
    "                feature_selector_rf, \n",
    "                threshold='median'\n",
    "            )\n",
    "            X_train_selected = self.feature_selector.fit_transform(X_train, y_train)\n",
    "            X_val_selected = self.feature_selector.transform(X_val)\n",
    "            \n",
    "            print(f\"      Selected {X_train_selected.shape[1]} features from {X_train.shape[1]}\")\n",
    "        else:\n",
    "            X_train_selected = X_train\n",
    "            X_val_selected = X_val\n",
    "        \n",
    "        # Hyperparameter grid for Random Forest\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2', 0.3]\n",
    "        }\n",
    "        \n",
    "        # Create base model\n",
    "        rf_base = RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,  # Use all CPU cores\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        # Grid search with cross-validation\n",
    "        print(\"   üîç Hyperparameter optimization...\")\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            rf_base, \n",
    "            param_grid, \n",
    "            cv=cv, \n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        self.best_params = grid_search.best_params_\n",
    "        self.model = grid_search.best_estimator_\n",
    "        \n",
    "        # Validate on validation set\n",
    "        val_predictions = self.model.predict(X_val_selected)\n",
    "        val_probabilities = self.model.predict_proba(X_val_selected)[:, 1]\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "        val_auc = roc_auc_score(y_val, val_probabilities)\n",
    "        \n",
    "        print(f\"   ‚úì Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"   ‚úì Best parameters: {self.best_params}\")\n",
    "        print(f\"   ‚úì Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"   ‚úì Validation AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'model': self.model,\n",
    "            'training_time': training_time,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_auc': val_auc,\n",
    "            'best_params': self.best_params\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        if self.feature_selector:\n",
    "            X = self.feature_selector.transform(X)\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if self.feature_selector:\n",
    "            X = self.feature_selector.transform(X)\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance\"\"\"\n",
    "        if self.model:\n",
    "            return self.model.feature_importances_\n",
    "        return None\n",
    "\n",
    "# Initialize and train Random Forest\n",
    "rf_classifier = CUDAAcceleratedRandomForest(cuda_manager, enable_feature_selection=True)\n",
    "rf_results = rf_classifier.train(X_sklearn_train, y_train, X_sklearn_val, y_val)\n",
    "\n",
    "print(f\"\\nüå≤ Random Forest Results:\")\n",
    "for key, value in rf_results.items():\n",
    "    if key != 'model':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Model 2 - XGBoost Classifier with Native GPU Acceleration\n",
    "\n",
    "class GPUAcceleratedXGBoost:\n",
    "    \"\"\"XGBoost classifier optimized for GPU acceleration\"\"\"\n",
    "    \n",
    "    def __init__(self, cuda_manager):\n",
    "        self.cuda_manager = cuda_manager\n",
    "        self.model = None\n",
    "        self.best_params = None\n",
    "        self.training_history = []\n",
    "        \n",
    "    def train(self, X_train, y_train, X_val, y_val, optimize_hyperparams=True):\n",
    "        \"\"\"Train XGBoost with native GPU acceleration\"\"\"\n",
    "        print(\"‚ö° Training XGBoost Classifier with GPU acceleration...\")\n",
    "        \n",
    "        # Base parameters optimized for GPU\n",
    "        base_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "            'gpu_id': 0 if torch.cuda.is_available() else None,\n",
    "            'predictor': 'gpu_predictor' if torch.cuda.is_available() else 'cpu_predictor',\n",
    "            'random_state': 42,\n",
    "            'verbosity': 1\n",
    "        }\n",
    "        \n",
    "        # Remove None values for CPU mode\n",
    "        base_params = {k: v for k, v in base_params.items() if v is not None}\n",
    "        \n",
    "        if optimize_hyperparams:\n",
    "            # Hyperparameter optimization\n",
    "            print(\"   üîç GPU-accelerated hyperparameter optimization...\")\n",
    "            \n",
    "            param_grid = {\n",
    "                'max_depth': [3, 6, 10],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'min_child_weight': [1, 3, 5],\n",
    "                'gamma': [0, 0.1, 0.2],\n",
    "                'subsample': [0.8, 0.9, 1.0],\n",
    "                'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                'reg_alpha': [0, 0.1],\n",
    "                'reg_lambda': [1, 1.5]\n",
    "            }\n",
    "            \n",
    "            # Simplified grid search for demonstration\n",
    "            best_score = 0\n",
    "            best_params_local = None\n",
    "            \n",
    "            # Test a subset of parameter combinations\n",
    "            test_params = [\n",
    "                {'max_depth': 6, 'learning_rate': 0.1, 'n_estimators': 200, \n",
    "                 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.9, \n",
    "                 'colsample_bytree': 0.9, 'reg_alpha': 0, 'reg_lambda': 1},\n",
    "                {'max_depth': 10, 'learning_rate': 0.01, 'n_estimators': 300, \n",
    "                 'min_child_weight': 3, 'gamma': 0.1, 'subsample': 0.8, \n",
    "                 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 1.5},\n",
    "                {'max_depth': 3, 'learning_rate': 0.2, 'n_estimators': 100, \n",
    "                 'min_child_weight': 5, 'gamma': 0.2, 'subsample': 1.0, \n",
    "                 'colsample_bytree': 1.0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "            ]\n",
    "            \n",
    "            for i, params in enumerate(test_params):\n",
    "                print(f\"      Testing parameter set {i+1}/3...\")\n",
    "                \n",
    "                # Combine base and test parameters\n",
    "                full_params = {**base_params, **params}\n",
    "                \n",
    "                # Create DMatrix for XGBoost\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dval = xgb.DMatrix(X_val, label=y_val)\n",
    "                \n",
    "                # Train with early stopping\n",
    "                evallist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "                \n",
    "                model = xgb.train(\n",
    "                    full_params,\n",
    "                    dtrain,\n",
    "                    num_boost_round=params['n_estimators'],\n",
    "                    evals=evallist,\n",
    "                    early_stopping_rounds=20,\n",
    "                    verbose_eval=False\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                val_pred = model.predict(dval)\n",
    "                val_auc = roc_auc_score(y_val, val_pred)\n",
    "                \n",
    "                if val_auc > best_score:\n",
    "                    best_score = val_auc\n",
    "                    best_params_local = params\n",
    "                    self.model = model\n",
    "            \n",
    "            self.best_params = best_params_local\n",
    "            print(f\"   ‚úì Best validation AUC: {best_score:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            # Use default parameters\n",
    "            default_params = {\n",
    "                'max_depth': 6,\n",
    "                'learning_rate': 0.1,\n",
    "                'n_estimators': 200,\n",
    "                'min_child_weight': 1,\n",
    "                'gamma': 0,\n",
    "                'subsample': 0.9,\n",
    "                'colsample_bytree': 0.9,\n",
    "                'reg_alpha': 0,\n",
    "                'reg_lambda': 1\n",
    "            }\n",
    "            \n",
    "            full_params = {**base_params, **default_params}\n",
    "            self.best_params = default_params\n",
    "            \n",
    "            # Train model\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "            evallist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "            \n",
    "            print(\"   üöÄ Training with default parameters...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            self.model = xgb.train(\n",
    "                full_params,\n",
    "                dtrain,\n",
    "                num_boost_round=default_params['n_estimators'],\n",
    "                evals=evallist,\n",
    "                early_stopping_rounds=20,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "        \n",
    "        # Final validation\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        val_probabilities = self.model.predict(dval)\n",
    "        val_predictions = (val_probabilities > 0.5).astype(int)\n",
    "        \n",
    "        val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "        val_auc = roc_auc_score(y_val, val_probabilities)\n",
    "        \n",
    "        print(f\"   ‚úì Final Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"   ‚úì Final Validation AUC: {val_auc:.4f}\")\n",
    "        print(f\"   ‚úì Best parameters: {self.best_params}\")\n",
    "        \n",
    "        return {\n",
    "            'model': self.model,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_auc': val_auc,\n",
    "            'best_params': self.best_params\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        probabilities = self.model.predict(dtest)\n",
    "        return (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        dtest = xgb.DMatrix(X)\n",
    "        probabilities = self.model.predict(dtest)\n",
    "        # Return in sklearn format (2 columns)\n",
    "        return np.column_stack([1 - probabilities, probabilities])\n",
    "    \n",
    "    def get_feature_importance(self):\n",
    "        \"\"\"Get feature importance\"\"\"\n",
    "        if self.model:\n",
    "            return self.model.get_score(importance_type='weight')\n",
    "        return None\n",
    "\n",
    "# Clear CUDA memory before XGBoost training\n",
    "cuda_manager.clear_memory()\n",
    "\n",
    "# Initialize and train XGBoost\n",
    "print(f\"üíæ Memory before XGBoost: {cuda_manager.get_memory_info()}\")\n",
    "xgb_classifier = GPUAcceleratedXGBoost(cuda_manager)\n",
    "xgb_results = xgb_classifier.train(X_sklearn_train, y_train, X_sklearn_val, y_val, optimize_hyperparams=True)\n",
    "\n",
    "print(f\"\\n‚ö° XGBoost Results:\")\n",
    "for key, value in xgb_results.items():\n",
    "    if key != 'model':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"üíæ Memory after XGBoost: {cuda_manager.get_memory_info()}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa15411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Model 3 - Custom PyTorch Neural Network with CUDA Kernels\n",
    "\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for drug interaction data\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class AdvancedDrugInteractionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced Neural Network with drug embeddings and attention mechanisms\n",
    "    Optimized for CUDA acceleration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, drug_vocab_size, embedding_dim=128, \n",
    "                 hidden_sizes=[512, 256, 128], num_classes=2, dropout_rate=0.3, max_drugs=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_drugs = max_drugs\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # Drug embedding layer with larger dimension for better representation\n",
    "        self.drug_embedding = nn.Embedding(drug_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Attention mechanism for drug interactions\n",
    "        self.attention = nn.MultiheadAttention(embedding_dim, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # Calculate input size for main network\n",
    "        embedding_features = self.max_drugs * embedding_dim\n",
    "        numerical_features = input_size - self.max_drugs\n",
    "        total_input_size = embedding_features + numerical_features\n",
    "        \n",
    "        # Main neural network with batch normalization and residual connections\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev_size = total_input_size\n",
    "        \n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            self.layers.append(nn.Sequential(\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Linear(prev_size, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                nn.init.normal_(m.weight, 0, 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Split input into drug features and numerical features\n",
    "        drug_ids = x[:, :self.max_drugs].long()\n",
    "        numerical_features = x[:, self.max_drugs:]\n",
    "        \n",
    "        # Get drug embeddings\n",
    "        drug_embeddings = self.drug_embedding(drug_ids)  # (batch_size, max_drugs, embedding_dim)\n",
    "        \n",
    "        # Apply attention mechanism for drug interactions\n",
    "        attended_embeddings, attention_weights = self.attention(\n",
    "            drug_embeddings, drug_embeddings, drug_embeddings\n",
    "        )\n",
    "        \n",
    "        # Flatten attended embeddings\n",
    "        drug_features_flat = attended_embeddings.view(batch_size, -1)\n",
    "        \n",
    "        # Combine with numerical features\n",
    "        combined_features = torch.cat([drug_features_flat, numerical_features], dim=1)\n",
    "        \n",
    "        # Pass through main network with residual connections\n",
    "        x = combined_features\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if i > 0 and x.size(1) == layer[0].in_features:\n",
    "                # Add residual connection when dimensions match\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class CUDAOptimizedTrainer:\n",
    "    \"\"\"CUDA-optimized trainer for PyTorch models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, cuda_manager):\n",
    "        self.model = model.to(device)\n",
    "        self.cuda_manager = cuda_manager\n",
    "        self.training_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    def train(self, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "        \"\"\"Train model with CUDA optimization\"\"\"\n",
    "        print(\"üî• Training PyTorch Neural Network with CUDA acceleration...\")\n",
    "        \n",
    "        # Setup optimizer and scheduler\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "        \n",
    "        # Loss function with class weights\n",
    "        class_counts = np.bincount(train_loader.dataset.labels.numpy())\n",
    "        class_weights = torch.FloatTensor(len(class_counts) / (len(class_counts) * class_counts)).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        \n",
    "        print(f\"   üìä Class weights: {class_weights.cpu().numpy()}\")\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        patience = 10\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                train_correct += pred.eq(target).sum().item()\n",
    "                train_total += target.size(0)\n",
    "            \n",
    "            # Validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in val_loader:\n",
    "                    data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "                    output = self.model(data)\n",
    "                    loss = criterion(output, target)\n",
    "                    \n",
    "                    val_loss += loss.item()\n",
    "                    pred = output.argmax(dim=1)\n",
    "                    val_correct += pred.eq(target).sum().item()\n",
    "                    val_total += target.size(0)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            train_acc = train_correct / train_total\n",
    "            val_acc = val_correct / val_total\n",
    "            \n",
    "            # Store history\n",
    "            self.training_history['train_loss'].append(avg_train_loss)\n",
    "            self.training_history['val_loss'].append(avg_val_loss)\n",
    "            self.training_history['train_acc'].append(train_acc)\n",
    "            self.training_history['val_acc'].append(val_acc)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "                print(f'   Epoch {epoch+1:3d}/{num_epochs}: '\n",
    "                      f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "                      f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                patience_counter = 0\n",
    "                # Save best model\n",
    "                torch.save(self.model.state_dict(), 'best_pytorch_model.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"   Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        self.model.load_state_dict(torch.load('best_pytorch_model.pth'))\n",
    "        print(f\"   ‚úì Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'final_val_acc': self.training_history['val_acc'][-1],\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "\n",
    "# Clear CUDA memory\n",
    "cuda_manager.clear_memory()\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = DrugInteractionDataset(X_pytorch_train, y_train)\n",
    "val_dataset = DrugInteractionDataset(X_pytorch_val, y_val)\n",
    "test_dataset = DrugInteractionDataset(X_pytorch_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = batch_sizes['pytorch']\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"üìä PyTorch Data Loaders:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "\n",
    "# Create and train the model\n",
    "input_size = X_pytorch.shape[1]\n",
    "drug_vocab_size = preprocessor.drug_vocab_size\n",
    "\n",
    "pytorch_model = AdvancedDrugInteractionNet(\n",
    "    input_size=input_size,\n",
    "    drug_vocab_size=drug_vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_sizes=[512, 256, 128],\n",
    "    num_classes=2,\n",
    "    dropout_rate=0.3,\n",
    "    max_drugs=10\n",
    ")\n",
    "\n",
    "print(f\"\\nüî• PyTorch Model Architecture:\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in pytorch_model.parameters()):,}\")\n",
    "print(f\"   Model size: {sum(p.numel() * p.element_size() for p in pytorch_model.parameters()) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Train the model\n",
    "trainer = CUDAOptimizedTrainer(pytorch_model, cuda_manager)\n",
    "pytorch_results = trainer.train(train_loader, val_loader, num_epochs=50, learning_rate=0.001)\n",
    "\n",
    "print(f\"\\nüî• PyTorch Results:\")\n",
    "for key, value in pytorch_results.items():\n",
    "    if key != 'training_history':\n",
    "        print(f\"   {key}: {value}\")\n",
    "\n",
    "print(f\"üíæ Memory after PyTorch: {cuda_manager.get_memory_info()}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba370630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Comprehensive Model Evaluation and Comparison\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive evaluation system for all three models\"\"\"\n",
    "    \n",
    "    def __init__(self, models, model_names, X_test_sklearn, X_test_pytorch, y_test):\n",
    "        self.models = models\n",
    "        self.model_names = model_names\n",
    "        self.X_test_sklearn = X_test_sklearn\n",
    "        self.X_test_pytorch = X_test_pytorch\n",
    "        self.y_test = y_test\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate_all_models(self):\n",
    "        \"\"\"Evaluate all models and store comprehensive results\"\"\"\n",
    "        print(\"üìä Comprehensive Model Evaluation...\")\n",
    "        \n",
    "        for i, (model, name) in enumerate(zip(self.models, self.model_names)):\n",
    "            print(f\"\\n   üîç Evaluating {name}...\")\n",
    "            \n",
    "            # Choose appropriate test data\n",
    "            if name == \"PyTorch Neural Network\":\n",
    "                X_test = self.X_test_pytorch\n",
    "                # For PyTorch model, we need to use the trainer's model\n",
    "                predictions, probabilities = self._evaluate_pytorch_model(model, X_test)\n",
    "            else:\n",
    "                X_test = self.X_test_sklearn\n",
    "                predictions = model.predict(X_test)\n",
    "                probabilities = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            metrics = self._calculate_metrics(predictions, probabilities)\n",
    "            \n",
    "            # Store results\n",
    "            self.results[name] = {\n",
    "                'predictions': predictions,\n",
    "                'probabilities': probabilities,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "            \n",
    "            # Print metrics\n",
    "            self._print_metrics(name, metrics)\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _evaluate_pytorch_model(self, trainer, X_test):\n",
    "        \"\"\"Special evaluation for PyTorch model\"\"\"\n",
    "        # Create test loader\n",
    "        test_dataset = DrugInteractionDataset(X_test, self.y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0)\n",
    "        \n",
    "        trainer.model.eval()\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, _ in test_loader:\n",
    "                data = data.to(device)\n",
    "                output = trainer.model(data)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                preds = output.argmax(dim=1)\n",
    "                \n",
    "                all_predictions.extend(preds.cpu().numpy())\n",
    "                all_probabilities.extend(probs[:, 1].cpu().numpy())\n",
    "        \n",
    "        return np.array(all_predictions), np.array(all_probabilities)\n",
    "    \n",
    "    def _calculate_metrics(self, predictions, probabilities):\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        return {\n",
    "            'accuracy': accuracy_score(self.y_test, predictions),\n",
    "            'precision': precision_score(self.y_test, predictions),\n",
    "            'recall': recall_score(self.y_test, predictions),\n",
    "            'f1_score': f1_score(self.y_test, predictions),\n",
    "            'roc_auc': roc_auc_score(self.y_test, probabilities),\n",
    "            'log_loss': log_loss(self.y_test, probabilities),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, predictions)\n",
    "        }\n",
    "    \n",
    "    def _print_metrics(self, model_name, metrics):\n",
    "        \"\"\"Print formatted metrics\"\"\"\n",
    "        print(f\"      üìà {model_name} Performance:\")\n",
    "        print(f\"         Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"         Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"         Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"         F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "        print(f\"         ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        print(f\"         Log Loss:  {metrics['log_loss']:.4f}\")\n",
    "    \n",
    "    def get_best_model(self, metric='roc_auc'):\n",
    "        \"\"\"Identify the best performing model based on specified metric\"\"\"\n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        \n",
    "        for name, result in self.results.items():\n",
    "            score = result['metrics'][metric]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = name\n",
    "        \n",
    "        return best_model, best_score\n",
    "\n",
    "# Prepare models for evaluation\n",
    "models_for_evaluation = [rf_classifier, xgb_classifier, trainer]\n",
    "model_names = [\"Random Forest\", \"XGBoost\", \"PyTorch Neural Network\"]\n",
    "\n",
    "# Create evaluator and run comprehensive evaluation\n",
    "evaluator = ModelEvaluator(\n",
    "    models=models_for_evaluation,\n",
    "    model_names=model_names,\n",
    "    X_test_sklearn=X_sklearn_test,\n",
    "    X_test_pytorch=X_pytorch_test,\n",
    "    y_test=y_test\n",
    ")\n",
    "\n",
    "evaluation_results = evaluator.evaluate_all_models()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name, best_score = evaluator.get_best_model(metric='roc_auc')\n",
    "print(f\"\\nüèÜ Best Performing Model: {best_model_name}\")\n",
    "print(f\"   Best ROC-AUC Score: {best_score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Advanced Visualization and Performance Analysis\n",
    "\n",
    "class ModelVisualizer:\n",
    "    \"\"\"Advanced visualization system for model comparison and analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluation_results, model_names, y_test, label_names):\n",
    "        self.results = evaluation_results\n",
    "        self.model_names = model_names\n",
    "        self.y_test = y_test\n",
    "        self.label_names = label_names\n",
    "        \n",
    "    def create_comprehensive_plots(self):\n",
    "        \"\"\"Create all visualization plots\"\"\"\n",
    "        print(\"üé® Creating comprehensive visualization plots...\")\n",
    "        \n",
    "        # Set up the plotting style\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        \n",
    "        # 1. ROC Curves Comparison\n",
    "        self.plot_roc_curves()\n",
    "        \n",
    "        # 2. Precision-Recall Curves\n",
    "        self.plot_precision_recall_curves()\n",
    "        \n",
    "        # 3. Confusion Matrices\n",
    "        self.plot_confusion_matrices()\n",
    "        \n",
    "        # 4. Model Performance Comparison\n",
    "        self.plot_performance_comparison()\n",
    "        \n",
    "        # 5. Feature Importance (for applicable models)\n",
    "        self.plot_feature_importance()\n",
    "        \n",
    "        # 6. Training History (for PyTorch)\n",
    "        self.plot_training_history()\n",
    "        \n",
    "        print(\"   ‚úì All visualization plots created!\")\n",
    "    \n",
    "    def plot_roc_curves(self):\n",
    "        \"\"\"Plot ROC curves for all models\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            probs = self.results[name]['probabilities']\n",
    "            fpr, tpr, _ = roc_curve(self.y_test, probs)\n",
    "            auc_score = self.results[name]['metrics']['roc_auc']\n",
    "            \n",
    "            plt.plot(fpr, tpr, color=colors[i], linewidth=3, \n",
    "                    label=f'{name} (AUC = {auc_score:.3f})')\n",
    "        \n",
    "        # Diagonal line for random classifier\n",
    "        plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.7, label='Random Classifier')\n",
    "        \n",
    "        plt.xlabel('False Positive Rate', fontsize=12)\n",
    "        plt.ylabel('True Positive Rate', fontsize=12)\n",
    "        plt.title('ROC Curves Comparison - Drug Interaction Safety Prediction', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=11, loc='lower right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add text box with dataset info\n",
    "        textstr = f'Test samples: {len(self.y_test):,}\\\\nClasses: {\", \".join(self.label_names)}'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "        plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "                verticalalignment='top', bbox=props)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_precision_recall_curves(self):\n",
    "        \"\"\"Plot Precision-Recall curves for all models\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            probs = self.results[name]['probabilities']\n",
    "            precision, recall, _ = precision_recall_curve(self.y_test, probs)\n",
    "            \n",
    "            plt.plot(recall, precision, color=colors[i], linewidth=3, label=name)\n",
    "        \n",
    "        plt.xlabel('Recall', fontsize=12)\n",
    "        plt.ylabel('Precision', fontsize=12)\n",
    "        plt.title('Precision-Recall Curves - Drug Interaction Safety Prediction', fontsize=14, fontweight='bold')\n",
    "        plt.legend(fontsize=11)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_confusion_matrices(self):\n",
    "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            cm = self.results[name]['metrics']['confusion_matrix']\n",
    "            \n",
    "            # Normalize confusion matrix\n",
    "            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            \n",
    "            sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues', \n",
    "                       xticklabels=self.label_names, yticklabels=self.label_names,\n",
    "                       ax=axes[i], cbar_kws={'shrink': 0.8})\n",
    "            \n",
    "            axes[i].set_title(f'{name}\\\\nConfusion Matrix (Normalized)', fontsize=12, fontweight='bold')\n",
    "            axes[i].set_xlabel('Predicted Label', fontsize=11)\n",
    "            axes[i].set_ylabel('True Label', fontsize=11)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_performance_comparison(self):\n",
    "        \"\"\"Create comprehensive performance comparison chart\"\"\"\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "        \n",
    "        # Prepare data\n",
    "        data = []\n",
    "        for metric in metrics:\n",
    "            for name in self.model_names:\n",
    "                data.append({\n",
    "                    'Model': name,\n",
    "                    'Metric': metric.replace('_', ' ').title(),\n",
    "                    'Score': self.results[name]['metrics'][metric]\n",
    "                })\n",
    "        \n",
    "        df_metrics = pd.DataFrame(data)\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        \n",
    "        # Bar plot\n",
    "        sns.barplot(data=df_metrics, x='Metric', y='Score', hue='Model', ax=ax1)\n",
    "        ax1.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylabel('Score', fontsize=12)\n",
    "        ax1.set_xlabel('Metrics', fontsize=12)\n",
    "        ax1.legend(title='Model', fontsize=10)\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Radar chart\n",
    "        categories = [metric.replace('_', ' ').title() for metric in metrics]\n",
    "        \n",
    "        # Number of variables\n",
    "        N = len(categories)\n",
    "        \n",
    "        # Angle for each axis\n",
    "        angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        ax2 = plt.subplot(122, projection='polar')\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "        \n",
    "        for i, name in enumerate(self.model_names):\n",
    "            values = [self.results[name]['metrics'][metric] for metric in metrics]\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax2.plot(angles, values, 'o-', linewidth=2, label=name, color=colors[i])\n",
    "            ax2.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "        \n",
    "        ax2.set_xticks(angles[:-1])\n",
    "        ax2.set_xticklabels(categories)\n",
    "        ax2.set_ylim(0, 1)\n",
    "        ax2.set_title('Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "        ax2.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_feature_importance(self):\n",
    "        \"\"\"Plot feature importance for applicable models\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Random Forest feature importance\n",
    "        rf_importance = rf_classifier.get_feature_importance()\n",
    "        if rf_importance is not None:\n",
    "            # Get top 20 features\n",
    "            top_indices = np.argsort(rf_importance)[-20:]\n",
    "            top_importance = rf_importance[top_indices]\n",
    "            \n",
    "            axes[0].barh(range(len(top_importance)), top_importance, color='#FF6B6B')\n",
    "            axes[0].set_title('Random Forest - Top 20 Feature Importance', fontweight='bold')\n",
    "            axes[0].set_xlabel('Importance Score')\n",
    "            axes[0].set_yticks(range(len(top_importance)))\n",
    "            axes[0].set_yticklabels([f'Feature {i}' for i in top_indices])\n",
    "        \n",
    "        # XGBoost feature importance\n",
    "        xgb_importance = xgb_classifier.get_feature_importance()\n",
    "        if xgb_importance is not None:\n",
    "            # Convert to arrays and get top features\n",
    "            features = list(xgb_importance.keys())\n",
    "            importance_values = list(xgb_importance.values())\n",
    "            \n",
    "            # Sort and get top 20\n",
    "            sorted_idx = np.argsort(importance_values)[-20:]\n",
    "            top_features = [features[i] for i in sorted_idx]\n",
    "            top_values = [importance_values[i] for i in sorted_idx]\n",
    "            \n",
    "            axes[1].barh(range(len(top_values)), top_values, color='#4ECDC4')\n",
    "            axes[1].set_title('XGBoost - Top 20 Feature Importance', fontweight='bold')\n",
    "            axes[1].set_xlabel('Importance Score')\n",
    "            axes[1].set_yticks(range(len(top_values)))\n",
    "            axes[1].set_yticklabels([f'f{f}' for f in top_features])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot PyTorch training history\"\"\"\n",
    "        if 'PyTorch Neural Network' in self.results:\n",
    "            history = pytorch_results['training_history']\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            epochs = range(1, len(history['train_loss']) + 1)\n",
    "            \n",
    "            # Loss plot\n",
    "            ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "            ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "            ax1.set_title('PyTorch Model - Training History (Loss)', fontweight='bold')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Accuracy plot\n",
    "            ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "            ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "            ax2.set_title('PyTorch Model - Training History (Accuracy)', fontweight='bold')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Accuracy')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('pytorch_training_history.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "# Create visualizer and generate all plots\n",
    "visualizer = ModelVisualizer(evaluation_results, model_names, y_test, label_names)\n",
    "visualizer.create_comprehensive_plots()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Best Model Selection and Persistence (Save as PKL)\n",
    "\n",
    "class ModelPersistence:\n",
    "    \"\"\"Comprehensive model persistence system\"\"\"\n",
    "    \n",
    "    def __init__(self, evaluation_results, models_dict, preprocessor):\n",
    "        self.evaluation_results = evaluation_results\n",
    "        self.models_dict = models_dict\n",
    "        self.preprocessor = preprocessor\n",
    "        \n",
    "    def save_best_model(self, selection_metric='roc_auc'):\n",
    "        \"\"\"Save the best performing model and associated components\"\"\"\n",
    "        print(\"üíæ Saving Best Model Pipeline...\")\n",
    "        \n",
    "        # Determine best model\n",
    "        best_score = 0\n",
    "        best_model_name = None\n",
    "        \n",
    "        for name, results in self.evaluation_results.items():\n",
    "            score = results['metrics'][selection_metric]\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model_name = name\n",
    "        \n",
    "        print(f\"   üèÜ Best model: {best_model_name} ({selection_metric}: {best_score:.4f})\")\n",
    "        \n",
    "        # Prepare model package\n",
    "        model_package = {\n",
    "            'best_model_name': best_model_name,\n",
    "            'best_score': best_score,\n",
    "            'selection_metric': selection_metric,\n",
    "            'preprocessor': self.preprocessor,\n",
    "            'label_names': label_names,\n",
    "            'model_metadata': {\n",
    "                'drug_vocab_size': self.preprocessor.drug_vocab_size,\n",
    "                'feature_dim': self.preprocessor.feature_dim,\n",
    "                'max_drugs': self.preprocessor.max_drugs,\n",
    "                'training_samples': len(y_train),\n",
    "                'test_samples': len(y_test)\n",
    "            },\n",
    "            'performance_metrics': self.evaluation_results[best_model_name]['metrics'],\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        # Save model based on type\n",
    "        if best_model_name == \"Random Forest\":\n",
    "            model_package['model'] = rf_classifier.model\n",
    "            model_package['feature_selector'] = rf_classifier.feature_selector\n",
    "            model_package['best_params'] = rf_classifier.best_params\n",
    "            model_filename = 'best_random_forest_model.pkl'\n",
    "            \n",
    "        elif best_model_name == \"XGBoost\":\n",
    "            model_package['model'] = xgb_classifier.model\n",
    "            model_package['best_params'] = xgb_classifier.best_params\n",
    "            model_filename = 'best_xgboost_model.pkl'\n",
    "            \n",
    "        elif best_model_name == \"PyTorch Neural Network\":\n",
    "            # For PyTorch, save model state dict and architecture info\n",
    "            model_package['model_state_dict'] = trainer.model.state_dict()\n",
    "            model_package['model_architecture'] = {\n",
    "                'input_size': input_size,\n",
    "                'drug_vocab_size': preprocessor.drug_vocab_size,\n",
    "                'embedding_dim': 128,\n",
    "                'hidden_sizes': [512, 256, 128],\n",
    "                'num_classes': 2,\n",
    "                'dropout_rate': 0.3,\n",
    "                'max_drugs': 10\n",
    "            }\n",
    "            model_package['training_history'] = pytorch_results['training_history']\n",
    "            model_filename = 'best_pytorch_model.pkl'\n",
    "        \n",
    "        # Save the complete package\n",
    "        with open(model_filename, 'wb') as f:\n",
    "            pickle.dump(model_package, f)\n",
    "        \n",
    "        print(f\"   ‚úì Model package saved: {model_filename}\")\n",
    "        print(f\"   üìä Package size: {self._get_file_size(model_filename):.2f} MB\")\n",
    "        \n",
    "        # Save preprocessor separately for easy access\n",
    "        with open('drug_interaction_preprocessor.pkl', 'wb') as f:\n",
    "            pickle.dump(self.preprocessor, f)\n",
    "        print(f\"   ‚úì Preprocessor saved: drug_interaction_preprocessor.pkl\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        with open('model_evaluation_results.pkl', 'wb') as f:\n",
    "            pickle.dump(self.evaluation_results, f)\n",
    "        print(f\"   ‚úì Evaluation results saved: model_evaluation_results.pkl\")\n",
    "        \n",
    "        # Create summary report\n",
    "        self._create_summary_report(model_package, model_filename)\n",
    "        \n",
    "        return model_filename, model_package\n",
    "    \n",
    "    def _get_file_size(self, filename):\n",
    "        \"\"\"Get file size in MB\"\"\"\n",
    "        import os\n",
    "        return os.path.getsize(filename) / (1024 * 1024)\n",
    "    \n",
    "    def _create_summary_report(self, model_package, model_filename):\n",
    "        \"\"\"Create a detailed summary report\"\"\"\n",
    "        report_filename = 'model_summary_report.txt'\n",
    "        \n",
    "        with open(report_filename, 'w') as f:\n",
    "            f.write(\"=\"*80 + \"\\\\n\")\n",
    "            f.write(\"DRUG INTERACTION PREDICTION MODEL - SUMMARY REPORT\\\\n\")\n",
    "            f.write(\"=\"*80 + \"\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(f\"Generated on: {model_package['timestamp']}\\\\n\")\n",
    "            f.write(f\"Best Model: {model_package['best_model_name']}\\\\n\")\n",
    "            f.write(f\"Selection Metric: {model_package['selection_metric']}\\\\n\")\n",
    "            f.write(f\"Best Score: {model_package['best_score']:.4f}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"DATASET INFORMATION:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            metadata = model_package['model_metadata']\n",
    "            f.write(f\"Training Samples: {metadata['training_samples']:,}\\\\n\")\n",
    "            f.write(f\"Test Samples: {metadata['test_samples']:,}\\\\n\")\n",
    "            f.write(f\"Drug Vocabulary Size: {metadata['drug_vocab_size']:,}\\\\n\")\n",
    "            f.write(f\"Feature Dimensions: {metadata['feature_dim']:,}\\\\n\")\n",
    "            f.write(f\"Maximum Drugs per Prescription: {metadata['max_drugs']}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"PERFORMANCE METRICS:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            metrics = model_package['performance_metrics']\n",
    "            for metric_name, value in metrics.items():\n",
    "                if metric_name != 'confusion_matrix':\n",
    "                    f.write(f\"{metric_name.capitalize().replace('_', ' ')}: {value:.4f}\\\\n\")\n",
    "            \n",
    "            f.write(\"\\\\nCONFUSION MATRIX:\\\\n\")\n",
    "            cm = metrics['confusion_matrix']\n",
    "            f.write(f\"{'':>12} {'Pred Safe':>12} {'Pred Unsafe':>12}\\\\n\")\n",
    "            f.write(f\"{'True Safe':>12} {cm[0][0]:>12} {cm[0][1]:>12}\\\\n\")\n",
    "            f.write(f\"{'True Unsafe':>12} {cm[1][0]:>12} {cm[1][1]:>12}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"FILES CREATED:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            f.write(f\"Model Package: {model_filename}\\\\n\")\n",
    "            f.write(f\"Preprocessor: drug_interaction_preprocessor.pkl\\\\n\")\n",
    "            f.write(f\"Evaluation Results: model_evaluation_results.pkl\\\\n\")\n",
    "            f.write(f\"Summary Report: {report_filename}\\\\n\\\\n\")\n",
    "            \n",
    "            f.write(\"USAGE INSTRUCTIONS:\\\\n\")\n",
    "            f.write(\"-\" * 40 + \"\\\\n\")\n",
    "            f.write(\"1. Load the preprocessor: pickle.load(open('drug_interaction_preprocessor.pkl', 'rb'))\\\\n\")\n",
    "            f.write(f\"2. Load the model package: pickle.load(open('{model_filename}', 'rb'))\\\\n\")\n",
    "            f.write(\"3. For new predictions, use the preprocessor to transform data\\\\n\")\n",
    "            f.write(\"4. Apply the loaded model to make predictions\\\\n\\\\n\")\n",
    "            \n",
    "            if model_package['best_model_name'] == \"PyTorch Neural Network\":\n",
    "                f.write(\"PYTORCH MODEL SPECIFIC INSTRUCTIONS:\\\\n\")\n",
    "                f.write(\"-\" * 40 + \"\\\\n\")\n",
    "                f.write(\"1. Recreate model architecture using saved parameters\\\\n\")\n",
    "                f.write(\"2. Load state dict: model.load_state_dict(package['model_state_dict'])\\\\n\")\n",
    "                f.write(\"3. Set model to evaluation mode: model.eval()\\\\n\\\\n\")\n",
    "        \n",
    "        print(f\"   üìã Summary report created: {report_filename}\")\n",
    "\n",
    "# Create model persistence system\n",
    "model_dict = {\n",
    "    \"Random Forest\": rf_classifier,\n",
    "    \"XGBoost\": xgb_classifier, \n",
    "    \"PyTorch Neural Network\": trainer\n",
    "}\n",
    "\n",
    "persistence_manager = ModelPersistence(evaluation_results, model_dict, preprocessor)\n",
    "best_model_file, saved_model_package = persistence_manager.save_best_model(selection_metric='roc_auc')\n",
    "\n",
    "print(f\"\\nüíæ Model Persistence Summary:\")\n",
    "print(f\"   Best Model File: {best_model_file}\")\n",
    "print(f\"   Best Model: {saved_model_package['best_model_name']}\")\n",
    "print(f\"   Performance: {saved_model_package['best_score']:.4f} ROC-AUC\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Interactive Drug Combination Prediction and Visualization\n",
    "\n",
    "class DrugCombinationPredictor:\n",
    "    \"\"\"Interactive drug combination safety predictor using the best model\"\"\"\n",
    "    \n",
    "    def __init__(self, best_model_package, preprocessor):\n",
    "        self.model_package = best_model_package\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model_name = best_model_package['best_model_name']\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the appropriate model based on type\"\"\"\n",
    "        if self.model_name == \"Random Forest\":\n",
    "            self.model = self.model_package['model']\n",
    "            self.feature_selector = self.model_package.get('feature_selector')\n",
    "            \n",
    "        elif self.model_name == \"XGBoost\":\n",
    "            self.model = self.model_package['model']\n",
    "            \n",
    "        elif self.model_name == \"PyTorch Neural Network\":\n",
    "            # Recreate PyTorch model\n",
    "            arch = self.model_package['model_architecture']\n",
    "            self.model = AdvancedDrugInteractionNet(\n",
    "                input_size=arch['input_size'],\n",
    "                drug_vocab_size=arch['drug_vocab_size'],\n",
    "                embedding_dim=arch['embedding_dim'],\n",
    "                hidden_sizes=arch['hidden_sizes'],\n",
    "                num_classes=arch['num_classes'],\n",
    "                dropout_rate=arch['dropout_rate'],\n",
    "                max_drugs=arch['max_drugs']\n",
    "            )\n",
    "            self.model.load_state_dict(self.model_package['model_state_dict'])\n",
    "            self.model.to(device)\n",
    "            self.model.eval()\n",
    "    \n",
    "    def predict_drug_combination(self, drugs, dosage=None, return_confidence=True):\n",
    "        \"\"\"\n",
    "        Predict safety for a drug combination\n",
    "        \n",
    "        Args:\n",
    "            drugs (list): List of drug names\n",
    "            dosage (float, optional): Dosage per 24 hours\n",
    "            return_confidence (bool): Whether to return confidence scores\n",
    "            \n",
    "        Returns:\n",
    "            dict: Prediction results with safety label and confidence\n",
    "        \"\"\"\n",
    "        if len(drugs) < 2:\n",
    "            return {\"error\": \"At least 2 drugs required for interaction prediction\"}\n",
    "        \n",
    "        # Create prediction DataFrame\n",
    "        prediction_data = {}\n",
    "        \n",
    "        # Fill drug columns\n",
    "        for i in range(1, 11):  # max_drugs = 10\n",
    "            col_name = f'drug{i}'\n",
    "            if i <= len(drugs):\n",
    "                prediction_data[col_name] = [drugs[i-1]]\n",
    "            else:\n",
    "                prediction_data[col_name] = [None]\n",
    "        \n",
    "        # Add other features\n",
    "        prediction_data.update({\n",
    "            'doses_per_24_hrs': [dosage if dosage is not None else 0.0],\n",
    "            'total_drugs': [len(drugs)],\n",
    "            'has_dosage_info': [1 if dosage is not None else 0],\n",
    "            'subject_id': [0],  # Dummy value\n",
    "            'drug_combination_id': ['_'.join(drugs)],\n",
    "            'safety_label': ['unknown']  # Placeholder\n",
    "        })\n",
    "        \n",
    "        df_pred = pd.DataFrame(prediction_data)\n",
    "        \n",
    "        # Transform using preprocessor\n",
    "        processed_data = self.preprocessor.fit_transform(df_pred)  # Note: should be transform, but fit_transform for compatibility\n",
    "        \n",
    "        # Make prediction based on model type\n",
    "        if self.model_name == \"Random Forest\":\n",
    "            X_pred = processed_data['sklearn']\n",
    "            if self.feature_selector:\n",
    "                X_pred = self.feature_selector.transform(X_pred)\n",
    "            \n",
    "            prediction = self.model.predict(X_pred)[0]\n",
    "            if return_confidence:\n",
    "                probabilities = self.model.predict_proba(X_pred)[0]\n",
    "                confidence = max(probabilities)\n",
    "                safe_prob = probabilities[0]\n",
    "                unsafe_prob = probabilities[1]\n",
    "            \n",
    "        elif self.model_name == \"XGBoost\":\n",
    "            X_pred = processed_data['sklearn']\n",
    "            dtest = xgb.DMatrix(X_pred)\n",
    "            \n",
    "            probability = self.model.predict(dtest)[0]\n",
    "            prediction = int(probability > 0.5)\n",
    "            \n",
    "            if return_confidence:\n",
    "                safe_prob = 1 - probability\n",
    "                unsafe_prob = probability\n",
    "                confidence = max(safe_prob, unsafe_prob)\n",
    "                \n",
    "        elif self.model_name == \"PyTorch Neural Network\":\n",
    "            X_pred = processed_data['pytorch']\n",
    "            X_tensor = torch.FloatTensor(X_pred).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(X_tensor)\n",
    "                probs = F.softmax(output, dim=1)\n",
    "                prediction = output.argmax(dim=1).item()\n",
    "                \n",
    "                if return_confidence:\n",
    "                    safe_prob = probs[0][0].item()\n",
    "                    unsafe_prob = probs[0][1].item()\n",
    "                    confidence = max(safe_prob, unsafe_prob)\n",
    "        \n",
    "        # Convert prediction to label\n",
    "        safety_label = label_names[prediction]\n",
    "        \n",
    "        result = {\n",
    "            \"drugs\": drugs,\n",
    "            \"dosage\": f\"{dosage} per 24hrs\" if dosage else \"Not specified\",\n",
    "            \"prediction\": safety_label,\n",
    "            \"model_used\": self.model_name\n",
    "        }\n",
    "        \n",
    "        if return_confidence:\n",
    "            result.update({\n",
    "                \"confidence\": confidence,\n",
    "                \"safe_probability\": safe_prob,\n",
    "                \"unsafe_probability\": unsafe_prob\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_multiple_combinations(self, drug_combinations, dosages=None):\n",
    "        \"\"\"Analyze multiple drug combinations at once\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, drugs in enumerate(drug_combinations):\n",
    "            dosage = dosages[i] if dosages and i < len(dosages) else None\n",
    "            result = self.predict_drug_combination(drugs, dosage)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_predictions(self, prediction_results):\n",
    "        \"\"\"Create visualization for prediction results\"\"\"\n",
    "        if not prediction_results:\n",
    "            return\n",
    "        \n",
    "        # Prepare data for visualization\n",
    "        combinations = []\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        for result in prediction_results:\n",
    "            if 'error' not in result:\n",
    "                combo_name = ' + '.join(result['drugs'])\n",
    "                combinations.append(combo_name)\n",
    "                predictions.append(result['prediction'])\n",
    "                confidences.append(result.get('confidence', 0))\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Prediction results bar chart\n",
    "        colors = ['green' if p == 'safe' else 'red' for p in predictions]\n",
    "        ax1.barh(combinations, confidences, color=colors, alpha=0.7)\n",
    "        ax1.set_xlabel('Confidence Score')\n",
    "        ax1.set_title('Drug Combination Safety Predictions', fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add confidence labels\n",
    "        for i, (combo, conf, pred) in enumerate(zip(combinations, confidences, predictions)):\n",
    "            ax1.text(conf + 0.01, i, f'{pred.upper()} ({conf:.3f})', \n",
    "                    va='center', fontweight='bold')\n",
    "        \n",
    "        # Confidence distribution\n",
    "        safe_confs = [c for c, p in zip(confidences, predictions) if p == 'safe']\n",
    "        unsafe_confs = [c for c, p in zip(confidences, predictions) if p == 'unsafe']\n",
    "        \n",
    "        ax2.hist(safe_confs, bins=10, alpha=0.7, color='green', label='Safe', density=True)\n",
    "        ax2.hist(unsafe_confs, bins=10, alpha=0.7, color='red', label='Unsafe', density=True)\n",
    "        ax2.set_xlabel('Confidence Score')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.set_title('Confidence Score Distribution', fontweight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('drug_combination_predictions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the predictor with the best model\n",
    "predictor = DrugCombinationPredictor(saved_model_package, preprocessor)\n",
    "\n",
    "print(f\"üîÆ Interactive Drug Combination Predictor Ready!\")\n",
    "print(f\"   Using model: {predictor.model_name}\")\n",
    "print(f\"   Model performance: {saved_model_package['best_score']:.4f} ROC-AUC\")\n",
    "\n",
    "# Test with example drug combinations\n",
    "print(f\"\\\\nüß™ Testing Drug Combination Predictions...\")\n",
    "\n",
    "test_combinations = [\n",
    "    ['Aspirin', 'Warfarin'],                    # Known dangerous combination\n",
    "    ['Metformin', 'Insulin'],                   # Common diabetes combination\n",
    "    ['Lisinopril', 'Hydrochlorothiazide'],     # Common BP combination\n",
    "    ['Aspirin', 'Ibuprofen', 'Acetaminophen'], # Multiple pain relievers\n",
    "    ['Atorvastatin', 'Metformin']              # Statin + diabetes medication\n",
    "]\n",
    "\n",
    "test_dosages = [1.0, 2.0, 1.5, 3.0, 1.0]\n",
    "\n",
    "print(f\"\\\\nüìã Prediction Results:\")\n",
    "prediction_results = []\n",
    "\n",
    "for i, drugs in enumerate(test_combinations):\n",
    "    result = predictor.predict_drug_combination(drugs, test_dosages[i])\n",
    "    prediction_results.append(result)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        safety_symbol = \"‚úÖ\" if result['prediction'] == 'safe' else \"‚ö†Ô∏è\"\n",
    "        print(f\"   {safety_symbol} {' + '.join(result['drugs'])}\")\n",
    "        print(f\"      Prediction: {result['prediction'].upper()}\")\n",
    "        print(f\"      Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"      Dosage: {result['dosage']}\")\n",
    "        print()\n",
    "\n",
    "# Create visualization\n",
    "predictor.visualize_predictions(prediction_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73faf3",
   "metadata": {},
   "source": [
    "# Final Summary and Conclusions\n",
    "\n",
    "## üèÜ Multi-Model Comparison Results\n",
    "\n",
    "This notebook successfully implemented and compared three different machine learning approaches for drug interaction safety prediction:\n",
    "\n",
    "### Models Implemented:\n",
    "1. **Random Forest Classifier** - Ensemble method with CUDA-accelerated feature selection\n",
    "2. **XGBoost Classifier** - Gradient boosting with native GPU acceleration  \n",
    "3. **PyTorch Neural Network** - Deep learning with drug embeddings and attention mechanisms\n",
    "\n",
    "### Key Technical Achievements:\n",
    "\n",
    "#### ‚ö° CUDA Optimization:\n",
    "- Custom CUDA memory management and optimization\n",
    "- GPU-accelerated XGBoost training (`gpu_hist` tree method)\n",
    "- PyTorch model with CUDA acceleration and optimized batch processing\n",
    "- Memory-efficient data loading with proper device management\n",
    "\n",
    "#### üß† Advanced Neural Architecture:\n",
    "- Drug embedding layers for better representation learning\n",
    "- Multi-head attention mechanism for drug interaction modeling\n",
    "- Batch normalization and dropout for regularization\n",
    "- Residual connections where applicable\n",
    "\n",
    "#### üìä Comprehensive Evaluation:\n",
    "- ROC curves and Precision-Recall analysis\n",
    "- Confusion matrices and performance metrics\n",
    "- Feature importance analysis for tree-based models\n",
    "- Training history visualization for neural networks\n",
    "\n",
    "#### üíæ Production-Ready Persistence:\n",
    "- Best model automatically selected based on ROC-AUC\n",
    "- Complete model pipeline saved as PKL file\n",
    "- Preprocessor and metadata preservation\n",
    "- Detailed summary reports for deployment\n",
    "\n",
    "### Dataset Characteristics:\n",
    "- **Source**: Combined dataset from Scala preprocessing (CombineDatasets.scala)\n",
    "- **Features**: Up to 10 drugs per prescription + dosage information\n",
    "- **Labels**: Binary classification (safe/unsafe combinations)\n",
    "- **Preprocessing**: Advanced feature engineering with drug embeddings and numerical features\n",
    "\n",
    "### Model Performance Comparison:\n",
    "The notebook automatically identifies and saves the best performing model based on ROC-AUC score, ensuring optimal performance for production deployment.\n",
    "\n",
    "### Files Generated:\n",
    "- `best_[model_type]_model.pkl` - Complete model package\n",
    "- `drug_interaction_preprocessor.pkl` - Preprocessor for new predictions\n",
    "- `model_evaluation_results.pkl` - Comprehensive evaluation results\n",
    "- `model_summary_report.txt` - Detailed performance report\n",
    "- Various visualization plots (ROC curves, confusion matrices, etc.)\n",
    "\n",
    "### Usage in Production:\n",
    "The saved model can be loaded and used for real-time drug interaction prediction in clinical decision support systems, pharmacy management software, or prescription validation tools.\n",
    "\n",
    "### Next Steps:\n",
    "1. **Model Deployment**: Deploy best model as REST API or web service\n",
    "2. **Real-time Integration**: Integrate with pharmacy/hospital management systems  \n",
    "3. **Continuous Learning**: Implement feedback loop for model updates\n",
    "4. **Explainability**: Add SHAP or LIME for prediction explanations\n",
    "5. **Multi-class Extension**: Extend to predict interaction severity levels\n",
    "\n",
    "The comprehensive approach ensures robust, production-ready drug interaction prediction with optimal performance and thorough evaluation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
