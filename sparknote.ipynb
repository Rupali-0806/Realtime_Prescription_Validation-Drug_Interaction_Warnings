{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# PySpark MLlib Multi-Model Drug Interaction Prediction\n",
    "\n",
    "This notebook implements and compares three different machine learning models using PySpark MLlib for drug interaction safety prediction:\n",
    "\n",
    "1. **Logistic Regression** - Binary classification with regularization\n",
    "2. **Random Forest Classifier** - Ensemble method for robust predictions\n",
    "3. **Gradient Boosted Trees (GBT)** - Advanced boosting algorithm\n",
    "\n",
    "## Key Features:\n",
    "- **PySpark MLlib**: Distributed machine learning at scale\n",
    "- **HDFS Integration**: Direct data loading from HDFS\n",
    "- **Comprehensive Evaluation**: Multiple metrics and visualization\n",
    "- **Model Comparison**: Side-by-side performance analysis\n",
    "\n",
    "## Dataset:\n",
    "- Source: HDFS path `hdfs://localhost:9000/output/combined_dataset_complete.csv`\n",
    "- Features: Drug combinations, dosage information, safety labels\n",
    "- Complete dataset processing with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Environment Setup and Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, concat_ws, isnan, count, mean, stddev\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# PySpark ML imports\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Visualization and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"‚è∞ Notebook started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-session-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Initialize Spark Session\n",
    "print(\"üöÄ Initializing Spark Session...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DrugInteractionMLlib\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úì Spark Session initialized successfully!\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Load Data from HDFS\n",
    "print(\"üìä Loading Drug Interaction Dataset from HDFS...\")\n",
    "print(\"   Source: hdfs://localhost:9000/output/combined_dataset_complete.csv\")\n",
    "\n",
    "hdfs_path = \"hdfs://localhost:9000/output/combined_dataset_complete.csv\"\n",
    "\n",
    "try:\n",
    "    # Load data from HDFS\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(hdfs_path)\n",
    "    \n",
    "    print(f\"   ‚úì Dataset loaded successfully!\")\n",
    "    print(f\"   Total records: {df.count():,}\")\n",
    "    print(f\"   Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Display schema\n",
    "    print(\"\\nüìã Dataset Schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nüìù Sample Records:\")\n",
    "    df.show(5, truncate=False)\n",
    "    \n",
    "    # Show statistics\n",
    "    print(\"\\nüìà Dataset Statistics:\")\n",
    "    df.groupBy(\"safety_label\").count().show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error loading dataset: {str(e)}\")\n",
    "    print(\"   Please ensure HDFS is running and the dataset exists at the specified path\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Data Preprocessing\n",
    "print(\"üîÑ Preprocessing data for MLlib models...\")\n",
    "\n",
    "# Identify drug columns (drug1 through drug10)\n",
    "drug_columns = [col_name for col_name in df.columns if col_name.startswith('drug') and col_name[4:].isdigit()]\n",
    "print(f\"   Found {len(drug_columns)} drug columns: {drug_columns}\")\n",
    "\n",
    "# Create a clean dataset with non-null values\n",
    "df_clean = df.na.drop(subset=['safety_label'])\n",
    "\n",
    "# Convert safety_label to numeric (0 for safe, 1 for unsafe)\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"label\",\n",
    "    when(col(\"safety_label\") == \"unsafe\", 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Label encoding complete (safe=0, unsafe=1)\")\n",
    "\n",
    "# Create numerical features\n",
    "# Handle total_drugs if it exists\n",
    "if 'total_drugs' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn('total_drugs', col('total_drugs').cast(DoubleType()))\n",
    "else:\n",
    "    # Count non-null drugs\n",
    "    from pyspark.sql.functions import coalesce\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'total_drugs',\n",
    "        sum([when(col(drug_col).isNotNull(), 1).otherwise(0) for drug_col in drug_columns])\n",
    "    )\n",
    "\n",
    "# Handle doses_per_24_hrs if it exists\n",
    "if 'doses_per_24_hrs' in df_clean.columns:\n",
    "    df_clean = df_clean.withColumn(\n",
    "        'doses_per_24_hrs_numeric',\n",
    "        when(col('doses_per_24_hrs').isNotNull(), col('doses_per_24_hrs').cast(DoubleType())).otherwise(0.0)\n",
    "    )\n",
    "    numerical_features = ['total_drugs', 'doses_per_24_hrs_numeric']\n",
    "else:\n",
    "    numerical_features = ['total_drugs']\n",
    "\n",
    "# Fill null values in numerical features\n",
    "for feature in numerical_features:\n",
    "    df_clean = df_clean.fillna({feature: 0.0})\n",
    "\n",
    "print(f\"   ‚úì Numerical features prepared: {numerical_features}\")\n",
    "\n",
    "# String indexing for drug columns (create indices for each drug)\n",
    "indexed_features = []\n",
    "indexers = []\n",
    "\n",
    "for drug_col in drug_columns[:3]:  # Use first 3 drug columns to keep feature space manageable\n",
    "    if drug_col in df_clean.columns:\n",
    "        # Fill null values with 'NONE'\n",
    "        df_clean = df_clean.fillna({drug_col: 'NONE'})\n",
    "        \n",
    "        indexer = StringIndexer(\n",
    "            inputCol=drug_col,\n",
    "            outputCol=f\"{drug_col}_index\",\n",
    "            handleInvalid=\"keep\"\n",
    "        )\n",
    "        indexers.append(indexer)\n",
    "        indexed_features.append(f\"{drug_col}_index\")\n",
    "\n",
    "print(f\"   ‚úì Drug indexing configured for {len(indexers)} columns\")\n",
    "\n",
    "# Combine all features\n",
    "feature_columns = numerical_features + indexed_features\n",
    "\n",
    "print(f\"   ‚úì Total feature columns: {len(feature_columns)}\")\n",
    "print(f\"\\n   Feature list: {feature_columns}\")\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nüìä Class Distribution:\")\n",
    "df_clean.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Train-Test Split\n",
    "print(\"üìÇ Splitting data into training and test sets...\")\n",
    "\n",
    "# Split data (80% training, 20% testing)\n",
    "train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_count = train_data.count()\n",
    "test_count = test_data.count()\n",
    "\n",
    "print(f\"   ‚úì Training set: {train_count:,} records ({train_count/(train_count+test_count)*100:.1f}%)\")\n",
    "print(f\"   ‚úì Test set: {test_count:,} records ({test_count/(train_count+test_count)*100:.1f}%)\")\n",
    "\n",
    "# Show label distribution in train and test\n",
    "print(\"\\n   Training set distribution:\")\n",
    "train_data.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"   Test set distribution:\")\n",
    "test_data.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model1-lr-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Model 1 - Logistic Regression\n",
    "print(\"ü§ñ Training Model 1: Logistic Regression\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build pipeline for Logistic Regression\n",
    "lr_pipeline_stages = indexers.copy()\n",
    "\n",
    "# Vector assembler\n",
    "lr_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "lr_pipeline_stages.append(lr_assembler)\n",
    "\n",
    "# Standard scaler for numerical stability\n",
    "lr_scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "lr_pipeline_stages.append(lr_scaler)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0,\n",
    "    family=\"binomial\"\n",
    ")\n",
    "lr_pipeline_stages.append(lr)\n",
    "\n",
    "# Create and fit pipeline\n",
    "lr_pipeline = Pipeline(stages=lr_pipeline_stages)\n",
    "\n",
    "print(\"   üîÑ Training Logistic Regression model...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "lr_model = lr_pipeline.fit(train_data)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"   ‚úì Model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "\n",
    "print(\"\\n   üìä Sample Predictions:\")\n",
    "lr_predictions.select(\"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model2-rf-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Model 2 - Random Forest Classifier\n",
    "print(\"üå≤ Training Model 2: Random Forest Classifier\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build pipeline for Random Forest\n",
    "rf_pipeline_stages = indexers.copy()\n",
    "\n",
    "# Vector assembler\n",
    "rf_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "rf_pipeline_stages.append(rf_assembler)\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=1,\n",
    "    seed=42\n",
    ")\n",
    "rf_pipeline_stages.append(rf)\n",
    "\n",
    "# Create and fit pipeline\n",
    "rf_pipeline = Pipeline(stages=rf_pipeline_stages)\n",
    "\n",
    "print(\"   üîÑ Training Random Forest model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model = rf_pipeline.fit(train_data)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"   ‚úì Model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "print(\"\\n   üìä Sample Predictions:\")\n",
    "rf_predictions.select(\"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Get feature importances\n",
    "rf_classifier = rf_model.stages[-1]\n",
    "feature_importances = rf_classifier.featureImportances\n",
    "print(f\"\\n   üìà Top Feature Importances:\")\n",
    "for i, importance in enumerate(feature_importances.toArray()[:10]):\n",
    "    if i < len(feature_columns):\n",
    "        print(f\"      {feature_columns[i]}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model3-gbt-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Model 3 - Gradient Boosted Trees\n",
    "print(\"üöÄ Training Model 3: Gradient Boosted Trees (GBT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Build pipeline for GBT\n",
    "gbt_pipeline_stages = indexers.copy()\n",
    "\n",
    "# Vector assembler\n",
    "gbt_assembler = VectorAssembler(\n",
    "    inputCols=feature_columns,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "gbt_pipeline_stages.append(gbt_assembler)\n",
    "\n",
    "# Gradient Boosted Trees model\n",
    "gbt = GBTClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=50,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "gbt_pipeline_stages.append(gbt)\n",
    "\n",
    "# Create and fit pipeline\n",
    "gbt_pipeline = Pipeline(stages=gbt_pipeline_stages)\n",
    "\n",
    "print(\"   üîÑ Training Gradient Boosted Trees model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gbt_model = gbt_pipeline.fit(train_data)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"   ‚úì Model trained in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "\n",
    "print(\"\\n   üìä Sample Predictions:\")\n",
    "gbt_predictions.select(\"label\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "# Get feature importances\n",
    "gbt_classifier = gbt_model.stages[-1]\n",
    "feature_importances = gbt_classifier.featureImportances\n",
    "print(f\"\\n   üìà Top Feature Importances:\")\n",
    "for i, importance in enumerate(feature_importances.toArray()[:10]):\n",
    "    if i < len(feature_columns):\n",
    "        print(f\"      {feature_columns[i]}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-models-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Model Evaluation and Metrics\n",
    "print(\"üìä Evaluating All Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize evaluators\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "multiclass_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\")\n",
    "\n",
    "models_info = [\n",
    "    (\"Logistic Regression\", lr_predictions),\n",
    "    (\"Random Forest\", rf_predictions),\n",
    "    (\"Gradient Boosted Trees\", gbt_predictions)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, predictions in models_info:\n",
    "    print(f\"\\nüîç Evaluating {model_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "    pr_auc = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderPR\"})\n",
    "    \n",
    "    accuracy = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"accuracy\"})\n",
    "    precision = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = multiclass_evaluator.evaluate(predictions, {multiclass_evaluator.metricName: \"f1\"})\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': auc,\n",
    "        'PR-AUC': pr_auc\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall:    {recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1:.4f}\")\n",
    "    print(f\"   ROC-AUC:   {auc:.4f}\")\n",
    "    print(f\"   PR-AUC:    {pr_auc:.4f}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüìà Model Comparison Summary:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_idx = results_df['ROC-AUC'].idxmax()\n",
    "best_model = results_df.loc[best_model_idx, 'Model']\n",
    "best_auc = results_df.loc[best_model_idx, 'ROC-AUC']\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model} (ROC-AUC: {best_auc:.4f})\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrices-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Confusion Matrices\n",
    "print(\"üìä Generating Confusion Matrices\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to compute confusion matrix from predictions\n",
    "def compute_confusion_matrix(predictions):\n",
    "    # Collect predictions and labels\n",
    "    pred_and_labels = predictions.select(\"prediction\", \"label\").collect()\n",
    "    \n",
    "    # Initialize confusion matrix\n",
    "    tp = fp = tn = fn = 0\n",
    "    \n",
    "    for row in pred_and_labels:\n",
    "        pred, label = row['prediction'], row['label']\n",
    "        if label == 1.0 and pred == 1.0:\n",
    "            tp += 1\n",
    "        elif label == 0.0 and pred == 1.0:\n",
    "            fp += 1\n",
    "        elif label == 0.0 and pred == 0.0:\n",
    "            tn += 1\n",
    "        elif label == 1.0 and pred == 0.0:\n",
    "            fn += 1\n",
    "    \n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "fig.suptitle('Confusion Matrices for All Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(models_info):\n",
    "    cm = compute_confusion_matrix(predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Safe', 'Unsafe'],\n",
    "                yticklabels=['Safe', 'Unsafe'])\n",
    "    axes[idx].set_title(model_name, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Confusion matrices saved as 'confusion_matrices.png'\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roc-curves-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: ROC Curves\n",
    "print(\"üìà Generating ROC Curves\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to compute ROC curve points\n",
    "def compute_roc_curve(predictions, num_points=100):\n",
    "    # Collect predictions and labels\n",
    "    pred_data = predictions.select(\n",
    "        col(\"label\").cast(\"double\").alias(\"label\"),\n",
    "        col(\"probability\").getItem(1).alias(\"probability\")\n",
    "    ).collect()\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    labels = np.array([row['label'] for row in pred_data])\n",
    "    probs = np.array([row['probability'] for row in pred_data])\n",
    "    \n",
    "    # Sort by probability\n",
    "    sorted_indices = np.argsort(probs)[::-1]\n",
    "    labels = labels[sorted_indices]\n",
    "    \n",
    "    # Calculate TPR and FPR\n",
    "    tpr_list = [0]\n",
    "    fpr_list = [0]\n",
    "    \n",
    "    total_positives = np.sum(labels == 1)\n",
    "    total_negatives = np.sum(labels == 0)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for label in labels:\n",
    "        if label == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        \n",
    "        tpr = tp / total_positives if total_positives > 0 else 0\n",
    "        fpr = fp / total_negatives if total_negatives > 0 else 0\n",
    "        \n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "    \n",
    "    return np.array(fpr_list), np.array(tpr_list)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, (model_name, predictions) in enumerate(models_info):\n",
    "    fpr, tpr = compute_roc_curve(predictions)\n",
    "    auc_score = results_df.loc[idx, 'ROC-AUC']\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.4f})',\n",
    "             color=colors[idx], linewidth=2)\n",
    "\n",
    "# Plot diagonal line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì ROC curves saved as 'roc_curves.png'\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-comparison-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12: Metrics Comparison Visualization\n",
    "print(\"üìä Generating Metrics Comparison Charts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create bar plots for each metric\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison Across Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(results_df['Model'], results_df[metric], \n",
    "                   color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Rotate x-axis labels\n",
    "    ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Metrics comparison saved as 'metrics_comparison.png'\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13: Feature Importance Visualization\n",
    "print(\"üìä Generating Feature Importance Charts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract feature importances from tree-based models\n",
    "rf_classifier = rf_model.stages[-1]\n",
    "gbt_classifier = gbt_model.stages[-1]\n",
    "\n",
    "rf_importances = rf_classifier.featureImportances.toArray()\n",
    "gbt_importances = gbt_classifier.featureImportances.toArray()\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Random Forest': rf_importances[:len(feature_columns)],\n",
    "    'GBT': gbt_importances[:len(feature_columns)]\n",
    "})\n",
    "\n",
    "# Sort by Random Forest importance\n",
    "importance_df = importance_df.sort_values('Random Forest', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('Feature Importance - Tree-Based Models', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Random Forest importance\n",
    "importance_df.plot(x='Feature', y='Random Forest', kind='barh', ax=ax1, \n",
    "                   color='#4ECDC4', legend=False)\n",
    "ax1.set_title('Random Forest Feature Importance', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Importance', fontsize=11)\n",
    "ax1.set_ylabel('Features', fontsize=11)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# GBT importance\n",
    "importance_df_gbt = importance_df.sort_values('GBT', ascending=False)\n",
    "importance_df_gbt.plot(x='Feature', y='GBT', kind='barh', ax=ax2, \n",
    "                       color='#45B7D1', legend=False)\n",
    "ax2.set_title('Gradient Boosted Trees Feature Importance', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Importance', fontsize=11)\n",
    "ax2.set_ylabel('Features', fontsize=11)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Feature importance saved as 'feature_importance.png'\")\n",
    "\n",
    "print(\"\\nüìã Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 14: Final Summary and Model Persistence\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ MODEL TRAINING AND EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Final Results Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüèÜ Best Performing Model: {best_model}\")\n",
    "print(f\"   ROC-AUC Score: {best_auc:.4f}\")\n",
    "\n",
    "print(\"\\nüìÅ Generated Files:\")\n",
    "print(\"   ‚úì confusion_matrices.png - Confusion matrices for all models\")\n",
    "print(\"   ‚úì roc_curves.png - ROC curve comparison\")\n",
    "print(\"   ‚úì metrics_comparison.png - Performance metrics comparison\")\n",
    "print(\"   ‚úì feature_importance.png - Feature importance analysis\")\n",
    "\n",
    "print(\"\\nüíæ Saving Best Model...\")\n",
    "best_model_path = f\"best_model_{best_model.replace(' ', '_').lower()}\"\n",
    "\n",
    "if best_model == \"Logistic Regression\":\n",
    "    lr_model.write().overwrite().save(best_model_path)\n",
    "elif best_model == \"Random Forest\":\n",
    "    rf_model.write().overwrite().save(best_model_path)\n",
    "else:\n",
    "    gbt_model.write().overwrite().save(best_model_path)\n",
    "\n",
    "print(f\"   ‚úì Best model saved to: {best_model_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚è∞ Analysis completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 15: Cleanup\n",
    "print(\"\\nüßπ Cleaning up resources...\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "\n",
    "print(\"‚úì Spark session stopped\")\n",
    "print(\"\\n‚úÖ All operations completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
